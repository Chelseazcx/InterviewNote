# 分布式缓存

## 理论

### CAP

- C（一致性）：所有的节点上的数据时刻保持同步
- A（可用性）：每个请求都能接受到一个响应，无论响应成功或失败
- P（分区容错）：系统应该能持续提供服务，即使系统内部有消息丢失（分区）

### ACID

- 原子性：一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成。
- 一致性：在事务开始之前和事务结束以后，数据库的完整性限制没有被破坏。
- 隔离性：当两个或者多个事务并发访问数据库的同一数据时所表现出的相互关系。事务隔离分为不同级别，包括读未提交(Read uncommitted)、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。
- 持久性：在事务完成以后，该事务对数据库所作的更改便持久地保存在数据库之中，并且是完全的。

### BASE

- Basically Available（基本可用）
- Soft state（软状态）
- Eventually consistent（最终一致性）

Raft具体过程如下：

- Client发起请求，每一条请求包含操作指令
- 请求交由Leader处理，Leader将操作指令(entry)追加(append)至操作日志，紧接着对Follower发起AppendEntries请求、尝试让操作日志副本在Follower落地
- 如果Follower多数派(quorum)同意AppendEntries请求，Leader进行commit操作、把指令交由状态机处理
- 状态机处理完成后将结果返回给Client

在Redis集群中用到类似Raft理论选取leader。

### MVCC

Mysql的InnoDB是这样做的：

- 引擎给每张表都增加2个字段，分别是create version和delete version；
- 插入操作时，记录的创建版本号就是事务版本号
- 更新操作时，采用的是先标记旧的那行记录为已删除，并且删除版本号是事务版本号，然后插入一行新记录的方式
- 删除操作时，就把事务版本号作为删除版本号

### Gossip

是一种去中心化思路，解决状态在集群中的传播和状态一致性的保证两个问题。

所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。

1. 状态的传播：A节点知道msg，传到集群中的部分节点(相邻节点)，后者再将msg传播到其余节点。每个节点最初只有部分信息，不断从其他节点收到Gossip，逐渐掌握整个集群状态信息
2. 对同一条状态信息，不同的节点可能掌握的值不同，通过gossip通信思路构建协议包版本得到解决

维护redis节点通信协议

## 分布式设计策略

### 心跳检测

![img](http://dl2.iteye.com/upload/attachment/0130/4084/8ae57200-65bd-33be-990e-308da033ee4b.jpg) 

**周期检测心跳机制**

Server端每间隔 t 秒向Node集群发起监测请求，设定超时时间，如果超过超时时间，则判断“死亡”。

**累计失效检测机制**

 在周期检测心跳机制的基础上，统计一定周期内节点的返回情况

### 高可用设计

**主备模式**

当主机宕机时，备机接管主机的一切工作，待主机恢复正常后，按使用者的设定以自动（热备）或手动（冷备）方式将服务切换到主机上运行。

**互备模式**

两台主机同时运行各自的服务工作且相互监测情况。比如分布式版本管理系统Git

**集群模式**

多个节点在运行，同时可以通过主控节点分担服务请求。

### 容错性

系统对于错误的包容能力，确切地说是容故障而非错误。

缓存失效雪崩问题也可以进行容错处理。比如，使用缓存通常都是先检查缓存是否存在，如果存在则直接返回缓存内容，如果不存在就直接查询数据库然后再缓存查询结果返回。因此，如果我们查询的某一些数据实际上不存在，就会造成每一次请求都查询DB，给数据库造成较大的压力。这种情况下，一个比较巧妙的方法是，将这个不存在的key预先设定一个值并存入缓存（过期时间不宜过长），避免大量请求透传到DB中。

### 负载均衡

使用多台集群服务器共同分担计算任务，把网络请求及计算分配到集群可用服务器上去

## 分布式设计实践

### 全局ID生成

- UUID：当前日期和时间，时钟序列，全局唯一的IEEE机器识别号，如网卡
- SnowFlake雪花算法：一个64位的二进制正整数，然后转换成10进制的数。依赖机器的时钟
- 数据库自增ID
- Redis：实现一个原子操作INCR和INCRBY递增的操作。当使用数据库性能不够时，可以采用Redis来代替，同时使用Redis集群来提高吞吐量。

### 哈希取模

通过可以描述记录的业务的id或key，通过hash函数计算，余数作为处理该数据的服务器索引编号。

来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致**大部分的请求过来，全部无法拿到有效的缓存**，导致大量的流量涌入数据库。

### 一致性哈希

一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。

如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据

一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成**缓存热点**的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。

![consistent-hashing-algorithm](https://github.com/orangehaswing/advanced-java/raw/master/images/consistent-hashing-algorithm.png)

### 虚拟桶(hash slot)

redis cluster 有固定的 16384个 hash slot，对每个 key计算 CRC16值，然后对 16384取模，可以获取 key 对应的 hash slot。

每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。

移动 hash slot 的成本是非常低的。任何一台机器宕机，另外两个节点不影响的。因为 key 找的是 hash slot，不是机器。

## Ehcache

主要特性

- Ehcache是最快的Java缓存之一，配置简单
- 支持多种缓存淘汰策略：LRU,LFU,FIFO
- 缓存数据有两级：内存和磁盘，因此无需担心容量问题
- 分布式缓存
- 具有缓存和缓存管理器的侦听接口：缓存管理器监听器；缓存时间监听器

### 过期策略

- FIFO：先进先出
- LFU：最少被使用，缓存的元素有一个hit属性，hit值最小的将会被清出缓存。
- LRU：最近最少使用，缓存的元素有一个时间戳，当缓存容量满了，而又需要腾出地方来缓存新的元素的时候，那么现有缓存元素中时间戳离当前时间最远的元素将被清出缓存。

### 瓶颈点

- 缓存漂移（cache drift）

  每个应用节点只管理自己的缓存，在更新某个节点的时候，不会影响到其他的节点，这样数据之间可能就不同步了。

- 数据库瓶颈

  对于单实例的应用来说，缓存可以保护数据库的读风暴；但是在集群环境中，每一个应用节点都要定期保持数据最新，节点越多，要维持这样的情况对数据库开销越大。

### 集群

**支持五种集群方案**

- Terracotta
- RMI
- JMS
- JGroups
- EhCache Server

其中的三种最为常用集群方式，分别是 RMI、JGroups 以及 EhCache Server 。

### RMI组播方式

![img](https://img-blog.csdn.net/20160803100135391?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

原理：当缓存改变时,ehcache会向组播IP地址和端口号发送RMI UDP组播包。
缺陷：Ehcache的组播做得比较初级,功能只是基本实现(比如简单的一个HUB,接两台单网卡的服务器,互相之间组播同步就没问题)，对一些复杂的环境(比如多台服务器,每台服务器上多地址，尤其是集群，存在一个集群地址带多个物理机，每台物理机又带多个虚拟站的子地址)，就容易出现问题。

### 2、P2P方式

原理：P2P要求每个节点的Ehcache都要指向其他的N-1个节点。

### 3、JMS消息模式

![img](https://img-blog.csdn.net/20160803100411044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**原理：**这种模式的核心就是一个消息队列，每个应用节点都订阅预先定义好的主题，同时，节点有元素更新时，也会发布更新元素到主题中去。各个应用服务器节点通过侦听MQ获取到最新的数据，然后分别更新自己的Ehcache缓存，Ehcache默认支持ActiveMQ，我们也可以通过自定义组件的方式实现类似Kafka，RabbitMQ。

### 4、Cache Server模式

原理：这种模式会存在主从节点。
![img](https://img-blog.csdn.net/20160803100507733?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**缺陷：**缓存容易出现数据不一致的问题，

### 适用场景

1. 比较少的数据表更新：ehcache会自动把缓存中关于此表的所有缓存都删除，这样可以达到同步，但是对于数据经常更新的表，失去缓存意义
2. 并发要求不是很严格
3. 对一致性不高

在实际工作中，更多是将Ehcache作为与Redis配合的二级缓存。
**第一种方式：**

![img](https://img-blog.csdn.net/20160803100655251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**注：**
这种方式通过应用服务器的Ehcache定时轮询Redis缓存服务器更同步更新本地缓存，缺点是因为每台服务器定时Ehcache的时间不一样，那么不同服务器刷新最新缓存的时间也不一样，会产生数据不一致问题，对一致性要求不高可以使用。

**第二种方式：**

![img](https://img-blog.csdn.net/20160803100815941?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**注：**
通过引入了MQ队列，使每台应用服务器的Ehcache同步侦听MQ消息，这样在一定程度上可以达到**准同步**更新数据，通过MQ推送或者拉取的方式，但是因为不同服务器之间的网络速度的原因，所以也不能完全达到强一致性。基于此原理使用Zookeeper等分布式协调通知组件也是如此。

## Redis

**5中存储类型**

- string
- list
- map（hash）
- set
- stored-set

**string类型**

1. 能表达3中类型：字符串、整数和浮点数。根据场景相互间自动转型，并且根据需要选取底层的承载方式
2. value内部以int、sds作为结构存储。int存放整型数据，sds存放字节/字符串和浮点型数据
3. sds内部结构： 
   - 用buf数组存储字符串的内容，但数组的长度会大于所存储内容的长度。会有一格专门存放”\0”（C标准库）作为结尾，还有预留多几个空的（即free区域），当append字符串的长度小于free区域，则sds不会重新申请内存，直接使用free区域
   - 扩容：当对字符串的操作完成后预期的串长度小于1M时，扩容后的buf数组大小=预期长度*2+1；若大于1M，则buf总是会预留出1M的free空间
   - value对象通常具有两个内存部分：redisObject部分和redisObject的ptr指向的sds部分。创建value对象时，通常需要为redisObject和sds申请两次内存。单对于短小的字符串，可以把两者连续存放，所以可以一次性把两者的内存一起申请了

**list类型**

1. list类型的value对象内部以linkedlist或ziplist承载。当list的元素个数和单个元素的长度较小时，redis会采用ziplist实现以减少内存占用，否则采用linkedlist结构
2. linkedlist内部实现是双向链表。在list中定义了头尾元素指针和列表的长度，是的pop/push操作、llen操作的复杂度为O(1)。由于是链表，lindex类的操作复杂度仍然是O(N)
3. ziplist的内部结构 
   - 所有内容被放置在连续的内存中。其中zlbytes表示ziplist的总长度，zltail指向最末元素，zllen表示元素个数，entry表示元素自身内容，zlend作为ziplist定界符
   - rpush、rpop、llen，复杂度为O(1);lpush/pop操作由于涉及全列表元素的移动，复杂度为O(N)

**map类型**

1. map又叫hash。map内部的key和value不能再嵌套map了，只能是string类型：整形、浮点型和字符串
2. map主要由hashtable和ziplist两种承载方式实现，对于数据量较小的map，采用ziplist实现
3. hashtable内部结构 
   - 主要分为三层，自底向上分别是dictEntry、dictht、dict
   - dictEntry：管理一个key-value对，同时保留同一个桶中相邻元素的指针，一次维护哈希桶的内部连
   - dictht：维护哈希表的所有桶链
   - dict：当dictht需要扩容/缩容时，用于管理dictht的迁移
   - 哈希表的核心结构是dictht，它的table字段维护着hash桶，它是一个数组，每个元素指向桶的第一个元素（dictEntry）
   - set值的流程：先通过MurmurHash算法求出key的hash值，再对桶的个数取模，得到key对应的桶，再进入桶中，遍历全部entry，判定是否已有相同的key，如果没有，则将新key对应的键值对插入到桶头，并且更新dictht的used数量，used表示hash表中已经存了多少元素。由于每次插入都要遍历hash桶中的全部entry，所以当桶中entry很多时，性能会线性下降
   - 扩容：通过负载因子判定是否需要增加桶数。负载因子=哈希表中已有元素/哈希桶数的比值。有两个阈值，小于1一定不扩容；大于5一定扩容。扩容时新的桶数目是现有桶的2n倍
   - 缩容：负载因子的阈值是0.1
   - 扩/缩容通过新建哈希表的方式实现。即扩容时，会并存两个哈希表，一个是源表，一个是目标表。通过将源表的桶逐步迁移到目标表，以数据迁移的方式实现扩容，迁移完成后目标表覆盖源表。迁移过程中，首先访问源表，如果发现key对应的源表桶已完成迁移，则重新访问目标表，否则在源表中操作
   - redis是单线程处理请求，迁移和访问的请求在相同线程内进行，所以不会存在并发性问题
4. ziplist内部结构 
   - 和list的ziplist实现类似。不同的是，map对应的ziplist的entry个数总是2的整数倍，奇数存放key，偶数存放value
   - ziplist实现下，由哈希遍历变成了链表的顺序遍历，复杂度变成O(N)

**set类型**

1. set以intset或hashtable来存储。hashtable中的value永远为null，当set中只包含整数型的元素时，则采用intset
2. intset的内部结构 
   - 核心元素是一个字节数组，从小到大有序存放着set的元素
   - 由于元素有序排列，所以set的获取操作采用二分查找方式实现，复杂度O(log(N))。进行插入时，首先通过二分查找得到本次插入的位置，再对元素进行扩容，再将预计插入位置之后的所有元素向右移动一个位置，最后插入元素，插入复杂度为O(N)。删除类似

**sorted-set类型**

1. 类似map是一个key-value对，但是有序的。value是一个浮点数，称为score，内部是按照score从小到大排序
2. 内部结构以ziplist或skiplist+hashtable来实现

**客户端与服务器的交互模式**

1. 串行的请求/响应模式 
   - 每一次请求的发送都依赖于上一次请求的相应结果完全接收，同一个连接的每秒吞吐量低
   - redis对单个请求的处理时间通常比局域网的延迟小一个数量级，所以串行模式下，单链接的大部分时间都处于网络等待
2. 双工的请求/相应模式(pipeline) 
   - 适用于批量的独立写入操作。即可将请求数据批量发送到服务器，再批量地从服务器连接的字节流中一次读取每个响应数据，减少了网络延迟，所以单连接吞吐量较串行会提高一个数量级
3. 原子化的批量请求/响应模式（事务） 
   - 客户端通过和redis服务器两阶段的交互做到批量命令原子执行的事务效果：入队操作（即服务器端先将客户端发送过来的连接对象暂存在请求队列中）和执行阶段（依次执行请求队列中的所有请求）
   - 一个连接的请求在执行批量请求的过程中，不会执行其他客户端的请求
   - redis的事务不是一致的，没有回滚机制。如果中途失败，则返回错误信息，但已经成功执行的命令不会回滚
   - 事务里面有可能会带有读操作作为条件，由于批量请求只会先入队列，再批量一起执行，所以一般读操作不会跟批量写请求一起执行，这时候就有可能会导致批量写之前和之后读到的数据不一致，这种可以通过乐观锁的可串行化来解决，redis通过watch机制实现乐观锁。具体实现过程看下一题
4. 发布/订阅模式 
   - 发布端和订阅者通过channel关联
   - channel的订阅关系，维护在reids实例级别，独立于redisDB的key-value体系。所有的channel都由一个map维护，键是channel的名字，value是它所有订阅者client的指针链表
5. 脚本化的批量执行（脚本模式）

**单线程处理的主要逻辑**

1. redis服务器对命令的处理都是单线程的，但是I/O层面却面向多个客户端并发地提供服务，并发到内部单线程的转化通过多路复用框架来实现
2. 首先从多路服用框架（epoll、evport、kqueue）中select出已经ready的文件描述符（fileDescriptor）
3. ready的标准是已有数据到达内核（kernel）、已准备好写入数据
4. 对于上一步已经ready的fd，redis会分别对每个fd上已ready的事件进行处理，处理完相同fd上的所有事件后，再处理下一个ready的fd。有3中事件类型 
   - acceptTcpHandler：连接请求事件
   - readQueryFromClient：客户端的请求命令事件
   - sendReplyToClient：将暂存的执行结果写回客户端
5. 对来自客户端的命令执行结束后，接下来处理定时任务（TimeEvent）
6. aeApiPoll的等待时间取决于定时任务处理（TimeEvent）逻辑
7. 本次主循环完毕，进入下一次主循环的beforeSleep逻辑，后者负责处理数据过期、增量持久化的文件写入等任务

**持久化机制**

1. redis主要提供了两种持久化机制：RDB和AOF；
2. RDB 
   - 默认开启，会按照配置的指定时间将内存中的数据快照到磁盘中，创建一个dump.rdb文件，redis启动时再恢复到内存中。
   - redis会单独创建fork()一个子进程，将当前父进程的数据库数据复制到子进程的内存中，然后由子进程写入到临时文件中，持久化的过程结束了，再用这个临时文件替换上次的快照文件，然后子进程退出，内存释放。
   - 需要注意的是，每次快照持久化都会将主进程的数据库数据复制一遍，导致内存开销加倍，若此时内存不足，则会阻塞服务器运行，直到复制结束释放内存；都会将内存数据完整写入磁盘一次，所以如果数据量大的话，而且写操作频繁，必然会引起大量的磁盘I/O操作，严重影响性能，并且最后一次持久化后的数据可能会丢失；
3. AOF 
   - 以日志的形式记录每个写操作（读操作不记录），只需追加文件但不可以改写文件，redis启动时会根据日志从头到尾全部执行一遍以完成数据的恢复工作。包括flushDB也会执行。
   - 主要有两种方式触发：有写操作就写、每秒定时写（也会丢数据）。
   - 因为AOF采用追加的方式，所以文件会越来越大，针对这个问题，新增了重写机制，就是当日志文件大到一定程度的时候，会fork出一条新进程来遍历进程内存中的数据，每条记录对应一条set语句，写到临时文件中，然后再替换到旧的日志文件（类似rdb的操作方式）。默认触发是当aof文件大小是上次重写后大小的一倍且文件大于64M时触发；
4. 当两种方式同时开启时，数据恢复redis会优先选择AOF恢复。一般情况下，只要使用默认开启的RDB即可，因为相对于AOF，RDB便于进行数据库备份，并且恢复数据集的速度也要快很多。
5. 开启持久化缓存机制，对性能会有一定的影响，特别是当设置的内存满了的时候，更是下降到几百reqs/s。所以如果只是用来做缓存的话，可以关掉持久化。

## 分布式Redis

### 水平拆分

- 数据分布：hash映射、范围映射、hash映射+范围映射

### 主备复制

**流程**

- 首先slave向master发起SYNC命令。这一步在slave启动后触发，master 被动的将新slave节点加入主备复制集群。
- master收到SYNC后，开启BGSAVE 操作。BGSAVE 是Redis的一种全量持久化机制（RDB）。
- BGSAVE 完成后，master将快照信息发送给slave。
- 发送期间，master收到来自Client的新写入命令，除了正常响应外，在存入一份到backlog队列。
- 快照信息发送完成后，master继续发送backlog队列中的信息。
- backlog发送完成之后，后续的操作同时发给slave，保持实时的异步复制。

对于上面的slave节点，处理逻辑如下：

- 发送完SYNC后，继续对外提供服务；

- 开始接受master的快照信息，此时会清空slave现有数据，并将master快照写入；

- 接收backlog 并执行（回放），期间对外提供读服务；

- 继续接受master的命令副本，并继续回放，从而保持和master的数据一致性。

  ​

**断点续传**

PSYNC，主从都维护一个offset记录当前已同步过的命令。

Redis的 PSYNC（Partial Sync）可以用于代替SYNC，做到master-slave基于断点续传的主备同步协议。master-slave 2端通过维护一个offset记录当前已经同步的命令，slave断开期间，master的增量命令会保存在缓存中。当slave重连后，告知master断开时的offset，master会将后续的数据继续同步，从而完成了断点续传。

### 故障转移

**sentinel的相互感知**

所有需要相互感知的sentinel都向他们共同的master节点上订阅相同的channel：__sentinel__:hello；新加入的sentinel节点向这个channel发布一条消息，包含了自己的信息，该channel的订阅者们就可以发现这个新的sentinel。随后新的sentinel和已有的其他sentinel建立长连接

![这里写图片描述](https://img-blog.csdn.net/20180811190943258?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eGlhbjkw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**master的故障发现**

sentinel节点通过定期的向master发送心跳包判断其存活状态，称为`PING`。一旦发现master没有正常的响应，sentinel将此master置为“`主观不可用`”。所谓“主观”，意思就是还未得到其他sentinel节点的确认

随后将 “主观不可用” 发送给其他所有的sentinel节点进行确认（`is-master-down-by-addr`），当确认的`sentinel节点数` >= `quorum`（可配置）时，则判定该master 为不可用（客观下线），随后进入failover流程。

**sentinel Leader选举**

最终只能有一个sentinel作为failover的发起者，此时需要开启一个Leader选举的过程，来确定谁来发起failover。

Redis的sentinel机制采用类似Raft协议实现这个选举算法：

- sentinelState的Epoch变量类似于Raft协议中的term（选举回合）。
- 每个确认了master“主观不可用”的sentinel节点，都会向周围广播自己的参选请求。
- 每个接收到参选请求的sentinel节点，如果还没人向它发送过参选请求。它就将本选举回合的意向置位首个参选sentinel并回复；如果已经表示过意向了，则拒绝其他的参选，并将自己的意向回复。
- 每个发送参选请求的sentinel，如果收到了超过一半的同意意向（投票者包含自己），则确定为Leader；如果本回合持续了足够长的时间还未选出Leader，则开启下一个回合。

**故障转移failover**

- 从slave节点中选出一个“合适的”节点作为新的master节点；
- 对上面的slave节点执行 salveof no one 命令，让其成为master节点；
- 向剩余的slave节点发送命令，让它们成为新的master的slave，复制规则和parallel-syncs参数有关；
- 更新对旧的master节点配置为slave，并保持对其关注，当其恢复后命令其去复制新master节点的内容。

**slave晋升的规则**

- 选择`slave-priority`（slave节点优先级）最高的节点，如果存在则返回，不存在则继续；
- 选择复制偏移量最大的slave节点（`复制最完整的`），如果存在则返回，不存在则继续；
- 选择runId最小的slave节点（`最先启动的`）。

### Redis Cluster

- 自动将数据进行分片，每个 master 上放一部分数据，不同节点数据无交集
- 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的

**基本通信原理**

redis cluster 节点间采用 gossip 协议进行通信 ， gossip 好处在于元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续，打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。

**分布式寻址算法**

- hash 算法（大量缓存重建）
- 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）
- redis cluster 的 hash slot 算法

**高可用与主备切换原理**

几乎跟哨兵是类似的

- 判断节点宕机
- 从节点过滤
- 从节点选举