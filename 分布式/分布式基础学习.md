# 分布式学习

# 初识Zookeeper

众所周知，zookeeper与分布式有着千丝万缕的联系。它虽然源自于hadoop，但目前zookeeper脱离hadoop的范畴开发分布式应用越来越普遍，想到这里，不禁为hadoop扼腕叹息。那么Zookeeper究竟是什么呢？从他的名字就可以看出来 ------ 动物园管理员，就相当于是把集群中的客户端当作许多小动物，管理员的作用就是维持他们的秩序，提供他们食物，当某小动物出现状况，如生病、脱毛等一系列现象，管理员自然会知道，并传达给其他动物这个消息，其他动物就不会再拉着这个小动物玩，会让它好好休息。好吧，这个例子十分生硬。简单来说，zookeeper = 通知机制 + 文件系统。**通知机制**

相当于上面那个例子，客户端注册监听它关心的目录节点，当目录节点发生变化，如数据改变、被删除、子目录节点增加删除时，zookeeper会通知客户端。 这时客户端就可以根据传过来的信息采取一系列的操作。**文件系统**
zookeeper维护一个如下图的文件结构
![img](https://pic2.zhimg.com/80/v2-509835693be5bee29463fd4f5ba46de1_hd.png)
每个子目录项如 NameService 都被称作为 znode，有如下四种类型：
PERSISTENT - 持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在PERSISTENT_SEQUENTIAL - 持久化顺序编号目录节点：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号EPHEMERAL - 临时目录节点：客户端与zookeeper断开连接后，该节点被删除
EPHEMERAL_SEQUENTIAL - 临时顺序编号目录节点：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号
2. znode 可以有子节点目录 (临时节点除外)，并且每个 znode 可以存储数据zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效时，znode 也就删除了znode可以被监控，实现上述通知机制
   我们能用Zookeeper做什么？统一命名服务：说白了，zookeeper会帮我们的文件起名，起的名字还挺好听，还不会重复，便于识别跟记忆，是不是很棒配置管理：简单点，改变一台机器的配置，其他机器也会跟着改变
   集群管理：监听是否有机器退出和加入、动态选举Master（最小节点法，最大数据法）队列管理1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，本文最后将实现此队列demo2、队列按照 FIFO 方式进行入队和出队操作。
   实现分布式锁，流程如下![img](https://pic1.zhimg.com/80/v2-a29a80b7b5760bd8008eaad1ea8497e0_hd.png)
   Zookeeper的基本概念**角色**1.领导者（Leader）：进行投票的发起和决议，更新系统状态2.学习者（Learner）跟随者（Follower）：接受客户端请求并向客户端返回结果，在选主过程中参与投票
   观察者（Observer）：接收客户端的连接，将写请求转发给leader节点。但Observer不参加投票，只同步leader状态。Observer的目的是为了扩展系统，提高读取速度
   3.客户端（Client）：请求发起方**session**zookeeper会为每个client分配一个session，类似于web服务器一样。针对session可以有保存一些关联数据Global session 全局session，在每个server上都存在
   local session 只在当前请求的server上存在，但只能进行读操作，要是要进行写操作，就得升级为全局session
   Zookeeper的工作原理zookeeper集群上每个server数据一致，leader在集群启动时选举，如图![img](https://pic2.zhimg.com/80/v2-5f72ed5c647014b48e85946846140f01_hd.png)
   写操作时，请求发给某server，再由server转发给leader，leader给每个server发送投票消息，每个server把投票结果传给leader，要是有半数server同意此请求，leader就会commit到每个服务器执行写操作，流程如下：![img](https://pic2.zhimg.com/80/v2-5dcab77de22d31fd4902deae6cd3d9d9_hd.png)
   写操作流程中，observer角色只负责转发请求，不参与投票，如图：![img](https://pic3.zhimg.com/80/v2-7249ddfa2b30da5389b37ae5f2e65ff6_hd.png)
   一个follower挂了，修复好之后会和leader通过一致性协议修复follower数据，达到每个server上数据最终一致存储数据时，过一段时间，zookeeper就会把所有server的数据镜像写出，然后把每个server上的数据删除，保证了每个server的容量在某一台follower写入了某数据的同时，读另一台follower刚刚写入的信息不一定成功，因为每台server数据同步会有少许间隔，所以说是最终一致性。不过session肯定是强一致性，通过修改数据后传回lastZxid来判断。若要实现强一致读，sync读两次实现实现，原理如下：![img](https://pic2.zhimg.com/80/v2-bafee33c1eeec16145e8ab8a2d10a6e1_hd.png)Watcher的特性：只通知改变事一次性、触发后失效、session内有效
   Zookeeper实现同步队列DemoDemo现实一种同步的分步式队列，当定义的队列成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。**实现思路**创建一个父目录 /queue，每个成员都监控(Watch)标志位目录/queue/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /queue/x(i)的临时目录节点，然后每个成员获取 /queue 目录的所有目录节点，也就是 x(i)。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /queue/start 的出现，如果已经相等就创建 /queue/start。`    public static void main(String[] args) throws Exception {
   ​     //模拟app1通过zk1提交x1,app2通过zk2提交x2,app3通过zk3提交x3
   ​     doAction(1);
   ​     doAction(2);
   ​     doAction(3);
    }


​    //以此集群的3台机器上加入某成员
​    public static void doAction(int client) throws Exception {
​        String host1 = "zookeeperServer1:2181";
​        String host2 = "zookeeperServer2:2181";
​        String host3 = "zookeeperServer3:2181";
​        ZooKeeper zk = null;
​        switch (client) {
​            case 1:
​                zk = connection(host1);
​                initQueue(zk);
​                joinQueue(zk, 1);
​                break;
​            case 2:
​                zk = connection(host2);
​                initQueue(zk);
​                joinQueue(zk, 2);
​                break;
​            case 3:
​                zk = connection(host3);
​                initQueue(zk);
​                joinQueue(zk, 3);
​                break;
​        }
​    }

​    // 创建一个与服务器的连接
​    public static ZooKeeper connection(String host) throws IOException {
​        ZooKeeper zk = new ZooKeeper(host, 60000, new Watcher() {
​            // 监控所有被触发的事件
​            public void process(WatchedEvent event) {
​                if (event.getType() == Event.EventType.NodeCreated && event.getPath().equals("/queue/start")) {
​                    System.out.println("Queue has Completed.Finish testing!!!");
​                }
​            }
​        });
​        return zk;
​    }

​    //初始化队列
​    public static void initQueue(ZooKeeper zk) throws KeeperException, InterruptedException {

​        System.out.println("WATCH => /queue/start");

​        //当这个znode节点被改变时，将会触发当前Watcher
​        zk.exists("/queue/start", true);

​        //如果/queue目录为空，创建此节点
​        if (zk.exists("/queue", false) == null) {
​            System.out.println("create /queue task-queue");
​            zk.create("/queue", "task-queue".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
​        } else {
​            System.out.println("/queue is exist!");
​        }
​    }

​    //成员加入队列
​    public static void joinQueue(ZooKeeper zk, int x) throws KeeperException, InterruptedException {
​        System.out.println("create /queue/x" + x + " x" + x);
​        zk.create("/queue/x" + x, ("x" + x).getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
​        isCompleted(zk);
​    }

​    //判断队列是否已满
​    public static void isCompleted(ZooKeeper zk) throws KeeperException, InterruptedException {
​        //规定队列大小
​        int size = 3;
​        //查询成员数
​        int length = zk.getChildren("/queue", true).size();
​        System.out.println("Queue Complete:" + length + "/" + size);
​        if (length >= size) {
​            System.out.println("create /queue/start start");
​            zk.create("/queue/start", "start".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
​        }
​    }
`控制台打印：
![img](https://pic3.zhimg.com/80/v2-21412900ccae3ef0201e0aedc3f8d5a2_hd.png)在linux上查看queue节点：![img](https://pic1.zhimg.com/80/v2-f6a91480c05e3fd543a5b0927321d5c0_hd.png)因为我们创建的任务节点是临时节点，而start节点是持久节点，所以最终我们查看queue节点时，仅有start节点存在，临时节点已经被zookeeper自动删除。

# Zookeeper实现分布式锁

分布式锁有三中实现方式：Zookeeper、Memcached、Redis 。本文先用Zookeeper实现一下方式1：利用名称唯一性谁能创建某名称节点，谁就获得锁。但释放锁时会出现惊群效应，被PASS方式2：利用临时顺序节点画了个思维导图如下：![img](https://pic3.zhimg.com/80/v2-2b655ff85d7f472aefada598e16d5252_hd.png)利用Zookeeper的EPHEMERAL_SEQUENTIAL类型节点及watcher机制，创建的节点都是有顺序的，每次选择最小编号的节点获得锁。当前节点watcher上一个节点，当上一个节点被删除时，因为是临时节点，也就是上一个节点与zookeeper断开连接时，当前节点成为最小节点，从而获得锁。关键代码：`//检查当前节点是否为最小节点
public boolean checkMinPath() throws KeeperException, InterruptedException {
​        //获取锁节点下所有待争夺节点
​        List subNodes = zk.getChildren(GROUP_PATH, false);
​        //对所有节点排序
​        Collections.sort(subNodes);
​        //判断此节点在所有节点中的位置
​        int index = subNodes.indexOf( selfPath.substring(GROUP_PATH.length()+1));
​        switch (index){
​            case -1:{
​                System.out.println(PREFIX_OF_THREAD+"本节点已不在了..."+selfPath);
​                return false;
​            }
​            case 0:{
​                System.out.println(PREFIX_OF_THREAD+"子节点中，我最小，可以获得锁了！哈哈"+selfPath);
​                return true;
​            }
​            default:{
​                this.waitPath = GROUP_PATH +"/"+ subNodes.get(index - 1);
​                System.out.println(PREFIX_OF_THREAD+"排在我前面的节点是 "+waitPath);
​                try{
​                    zk.getData(waitPath, true, new Stat());
​                    return false;
​                }catch(KeeperException e){
​                    if(zk.exists(waitPath,false) == null){
​                        System.out.println(PREFIX_OF_THREAD+"排在我前面的"+waitPath+"已消失 ");
​                        return checkMinPath();
​                    }else{
​                        throw e;
​                    }
​                }
​            }
​        }
​    }`

# 初识Nginx

Nginx两大作用：1.轻量web容器 2.反向代理服务器。

在Java领域中，Nginx更多的被用作在前端提供负载均衡的反向代理服务。

所以本文仅简要介绍下反向代理概念和负载均衡的实现。

.

## 反向代理

是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。

随便画了个流程图如下：

![img](https://pic4.zhimg.com/80/v2-5dd5e71abf6f3d25f33014303c9322cf_hd.png)

## 安装Nginx

网上教程一大堆，略过了......

.

## 常用命令

nginx -s stop 快速关闭Nginx，可能不保存相关信息，并迅速终止web服务。

nginx -s quit 平稳关闭Nginx，保存相关信息，有安排的结束web服务。

nginx -s reload 因改变了Nginx相关配置，需要重新加载配置而重载。

nginx -s reopen 重新打开日志文件。

nginx -c filename 为 Nginx 指定一个配置文件，来代替缺省的。

nginx -t 不运行，而仅仅测试配置文件。nginx 将检查配置文件的语法的正确性，并尝试打开配置文件中所引用到的文件。

nginx -v 显示 nginx 的版本。

nginx -V 显示 nginx 的版本，编译器版本和配置参数。

.

## Demo配置

先以一个小Demo来了解一下它是怎么实现负载均衡吧。

如上面那个流程图所示，我们想要通过访问Nginx，经过负载均衡，访问到不同Tomcat下的某项目（如[个人博客](https://link.zhihu.com/?target=http%3A//kkys.online/blog)）。

配置文件如下：

```
http {
    #设定mime类型,类型由mime.type文件定义
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    #设定日志格式
    access_log    /var/log/nginx/access.log;

    #设定负载均衡的服务器列表
    upstream load_balance_server {
        #weigth参数表示权值，权值越高被分配到的几率越大
        server 127.0.0.1        weight=2;
        server 127.0.0.1:8081   weight=1;
    }

   #HTTP服务器
   server {
        #监听80端口
        listen       8090;
        
        #以什么IP进行访问
        server_name  localhost;

        #对所有请求进行负载均衡请求
        location /blog {
            #请求转向load_balance_server 定义的服务器列表
            proxy_pass  http://load_balance_server ;
        }
    }
}

```

配置文件中，我们通过定义了一个upstream来说明负载均衡策略（详情见下文），然后在location中对应。

配置完后执行nginx -s reload命令来重启Nginx.

因为我们的Nginx的端口是8090，,所以我们访问[http://kkys.online:8090/blog](https://link.zhihu.com/?target=http%3A//kkys.online%3A8090/blog)时，nginx通过负载均衡，会按权重跳到我们在upstream中设置的地址。

如下图所示：

![img](https://pic3.zhimg.com/80/v2-a503033820ba9be668f5465ebe5f2562_hd.png)

说明测试通过，但因博客中的图片不是全局路径，所以会显示不出来。通过原网址[http://kkys.online/blog](https://link.zhihu.com/?target=http%3A//kkys.online/blog)访问没有问题。

.

## Nginx负载均衡调度的方法

即上面配置中的upstream可选的四种负载均衡算法

- 服务器轮询（默认方式）：每个请求访问按照时间顺序逐一分配到不同的服务器端，如果后端某台服务器宕机时，故障系统会被自动的剔除，使用户访问不受影响。Weight（权重）指定轮询的权值，Weight值越大，分配到的访问几率越高，主要用于服务器端性能不均的情况下。
- ip_hash：每个请求按照访问的IP的Hash值进行分配，这行来自同一个IP的用户将会固定到后端的一个服务器，固定服务器后可以有效的解决网页存在的session共享的问题。
- fair：该算法可以根据页面大小和加载时间长短智能的进行决策负载均衡，即根据后端服务器的响应时间来分配请求，响应时间段的优先分配。Nginx本身未集成fair模块，如果需要该调度算法，必须下载Nginx的upstream_fair模块，然后在config中配置加载。
- url_hash：此调度算法是根据访问的url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步的提高后端服务器的效率。Nginx本身未集成该模块，如果使用需安装Nginx的hash包，并编译加载到nginx。

## Nginx的upstream模块支持的状态参数

在http的upstream模块中，可以通过server指令指定后端服务器的IP地址和端口，同时还可以设定每个后端服务器在负载均衡调度中的状态。通常设置的状态参数如下：

- down：表示当前的server暂时不参与负载均衡。
- backup：预留的备份服务器。当其他的所有非backup机器出现故障或者忙的时间，才会请求backup服务器，因此这台服务器的压力最轻。
- max_fails：允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。
- fail_timeout：在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。

# Spring Session + Redis实现分布式Session共享

单个服务器的应用，Tomcat会将Session保存在本机内存中，但一旦涉及到分布式应用，如何实现不同服务器间的Session共享问题呢？

目前比较主流的方式还是基于分布式缓存Memcached、redis实现，具体实现方式也有多种，像是：

- memcached-session-manager
- tomcat-redis-session-manager
- Spring Session

区别就是前两种需要依靠Tomcat，而Spring Session并不依靠容器，可扩展性更强，所以本文先用Redis + Spring Session实现一遍。

## 放上Demo

1.添加pom

```
<dependency>  
          <groupId>org.springframework.session</groupId>  
          <artifactId>spring-session-data-redis</artifactId>  
          <version>1.2.1.RELEASE</version>  
</dependency>  
<dependency>  
          <groupId>redis.clients</groupId>  
          <artifactId>jedis</artifactId>  
          <version>2.8.1</version>  
</dependency>  

```

2.Spring配置

```
<!-- 将session放入redis -->
<bean class="org.springframework.session.data.redis.config.annotation.web.http.RedisHttpSessionConfiguration"/>

<bean class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory">
    <property name="hostName" value="localhost" />
    <property name="password" value="your-password" />
    <property name="port" value="6379" />
    <property name="database" value="10" />
</bean>

```

3.配置web.xml过滤器

```
<filter>  
    <filter-name>springSessionRepositoryFilter</filter-name>  
    <filter-class>org.springframework.web.filter.DelegatingFilterProxy</filter-class>  
</filter>  
<filter-mapping>  
    <filter-name>springSessionRepositoryFilter</filter-name>  
    <url-pattern>/*</url-pattern>  
</filter-mapping>  

```

大功告成，是不是相当简单。第二步配置spring session + redis模板，第三步配置session过滤器，这样就把服务器中的session缓存到redis中了，实现了分布式session的共享。

最后，在servlet或者action里面添加一个session，再去redis中查看一下是否有这个session就好，这里暂不测试了。

spring session可以理解是替换了Servlet那一套会话管理，既不依赖容器，又不需要改动代码，并且是用了spring-data-redis那一套连接池，可以说是比较好的解决方案。

# 初识消息系统kafka

消息系统在分布式应用中有着不可或缺的地位，像是成产消费数据解耦，缓存未处理的消息等等。
那为什么不学习用Java写的ActiveMQ或RabbitMQ呢？因为我看过卡夫卡写的变形记。**简单原理图**![img](https://pic1.zhimg.com/80/v2-de11aa808238378ed4130e4e97e35f58_hd.png)
分布式消息系统就是生产者集群和消费者集群分离，通过中间的一个消息系统进行通信。生产者异步生产东西，不用管消费者的反馈，消费者也不用死等着生产者生产，等有东西了来拿就好。就像是母鸡下蛋，母鸡（生产者）直接把蛋（消息）下在筐里，人（消费者）不用在一边等着，只用隔段时间来拿就行了。**概念介绍**producerKafka系统中的生产者，用于产生数据并发送给broker进行存储。由于需要与broker中的分区保持socket连接，因此需要在zk中维护生产者与分区broker的对应关系。同一个topic下的数据，会以某种负载均衡的方式发送到不同的分区中。brokerBroker可以当做Kafka中的存储节点，数据按照topic组织，按照某种负载均衡方式分配到不同的分区中。一个Topic由多个分区组成，每个分区可以设置备份数量。分区由一个leader+多个followers组成，生产者直接与leader进行沟通，leader接收消息后，其他的followers会同步这个消息。所有的follwers同步消息后，该消息才会成为可消费的状态。Broker中Topic与分区，分区与生产者，分区之间的选举备份等等信息都需要zookeeper进行协调。consumerConsumer是Kafka中的消费者，通常以组的形式存在，一个Group会包含多个Consumer。每个组对应一个Topic，该Topic内的分区只能对应一个消费者，也就是如果消费者很多的情况下，会出现有的消费者消费不到数据；如果消费者很少的情况下，会有消费者同时消费多个分区的数据。Kafka仅仅会保证一个分区的消息的消费是有序的，多个分区并不保证有序性。为了保证数据消费的可靠性，Kakka提供了几种消费的机制：1 at most once，即消费数据后，保存offset，就再也取不到这个数据了。2 at least once，即消费数据后，保存offset，如果保存出错，下次可能还会取到该数据在Kafka中offset是由consumer维护的（实际可以由zookeeper来完成）。这种机制有两个好处，一个是可以依据consumer的能力来消费数据，避免产生消费数据的压力；另一个就是可以自定义fetch消费的数据数目，可以一次读取1条，也可以1次读取100条。topicKafka中的数据的主题，所有的操作（如消息的存储和读取\消费）都是依据topic完成。partition每个Topic由多个分区组成，每个分区内部的数据保证了有序性，即是按照时间序列，append到分区的尾部。分区是有固定大小的，容量不够时，会创建新的分区。Kafka在一定时间内会定期清理过期的文件。这种连续性的文件存储，一方面有效的利用磁盘的线性存取；另一方面减轻了内存的压力。zookeeper在Kafka中很多节点的调度以及资源的分配，都要依赖于zookeeper来完成。
1 Broker的注册，保存Broker的IP以及端口；2 Topic注册，管理broker中Topic的分区以及分布情况3 Broker的负载均衡，讲Topic动态的分配到broker中，通过topic的分布以及broker的负载判断4 消费者，每个分区的消息仅发送给一个消费者5 消费者与分区的对应关系，存储在zk中6 消费者负载均衡，一旦消费者增加或者减少，都会触发消费者的负载均衡7 消费者的offset，High level中由zk维护offset的信息；Low Level中由自己维护offsetDemo实现由于我租的乞丐版服务器，开伪集群有一些困难，所以以下demo均在单机上完成。首先来看一下命令行实现简易demo：启动kafka服务

`bin/kafka-server-start.sh config/server.properties &
`创建一个Topic，一个分区，一个备份

`bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test`启动生产者控制台

`bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test `另开一个窗口，启动消费者控制台

`bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
`在生产者控制台中生产数据

![img](https://pic3.zhimg.com/80/v2-4fa4185072badb94977cb7378e70de7a_hd.png)这时换到消费者控制台，显示如下，表示数据已经被消费

![img](https://pic1.zhimg.com/80/v2-a6eb7aed807838a1b296d133d83024c4_hd.png)**用kafka的Java-api实现**maven`

​     org.apache.kafka
​     kafka-clients
​     0.10.0.1
`生产者`public class Produce {

​    public static void main(String[] args) {
​        System.out.println("begin produce");
​        connectionKafka();
​        System.out.println("finish produce");
​    }

​    public static void connectionKafka() {
​        Properties props = new Properties();
​        props.put("bootstrap.servers", "your url:9092");
​        props.put("retries", 0);
​        props.put("batch.size", 16384);
​        props.put("buffer.memory", 33554432);
​        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
​        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

​        Producer producer = new KafkaProducer<>(props);
​        for (int i = 0; i < 10; i++) {
​            producer.send(new ProducerRecord("test2", Integer.toString(i), Integer.toString(i)));
​            System.out.println("send成功");
​            try {
​                Thread.currentThread().sleep(2000);
​            } catch (InterruptedException e) {
​                e.printStackTrace();
​            }
​        }
​        producer.close();
​    }
}
`消费者`public class Consumer{

​    public static void main(String[] args) {
​        Properties props = new Properties();
​        props.put("bootstrap.servers", "your url:9092");
​        props.put("group.id", "test");
​        props.put("enable.auto.commit", "true");
​        props.put("auto.commit.interval.ms", "1000");
​        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
​        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
​        KafkaConsumer consumer = new KafkaConsumer<>(props);
​        consumer.subscribe(Arrays.asList("test2"));
​        while (true) {
​            ConsumerRecords records = consumer.poll(100);
​            for (ConsumerRecord record : records)
​                System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
​        }
​    }
}`先运行消费者，再运行生产者。可以看到生产者2秒生成一条数据，随即发送给消息队列，消费者轮询，有消息来就消费。这种属于逐条发送，略微影响效率。也可以把生产者消息先放到缓存队列中，到达一定的数量一起发送，通过设置配置文件中的linger.ms参数大于0来实现。
控制台打印如下![img](https://pic4.zhimg.com/80/v2-91c5249db7141a16957a0aae49903783_hd.png)![img](https://pic4.zhimg.com/80/v2-8b467c506631504f65640a70c1cfbf77_hd.png)

# 深入kafka生产消费模型

## 生产者详解

上文中生产者代码如下：

```
Properties props = new Properties();
	 props.put("bootstrap.servers", "localhost:9092");
	 props.put("acks", "all");
	 props.put("retries", 0);
	 props.put("batch.size", 16384);
	 props.put("linger.ms", 1);
	 props.put("buffer.memory", 33554432);
	 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
	 props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
	 Producer<String, String> producer = new KafkaProducer<>(props);
	 for(int i = 0; i < 100; i++)
		 producer.send(new ProducerRecord<String, String>("my-topic", Integer.toString(i), Integer.toString(i)));
	 producer.close();
```

- 生产者的缓冲空间池保留尚未发送到服务器的消息，后台I/O线程负责将这些消息转换成请求发送到集群。如果使用后不关闭生产者，则会泄露这些资源。

- ```
  public Future<RecordMetadata> send(ProducerRecord<K,V> record,Callback callback)
  ```

- send()方法是异步的，添加消息到缓冲区等待发送，并立即返回。生产者将单个的消息批量在一起发送来提高效率。回调参数可选。send是异步的，并且一旦消息被保存在等待发送的消息缓存中，此方法就立即返回。这样并行发送多条消息而不阻塞去等待每一条消息的响应。发送的结果是一个RecordMetadata，它指定了消息发送的分区，分配的offset和消息的时间戳。

- ack是判别请求是否为完整的条件（就是是判断是不是成功发送了）。我们指定了“all”将会阻塞消息，这种设置性能最低，但是是最可靠的。

- retries，如果请求失败，生产者会自动重试，我们指定是0次，如果启用重试，则会有重复消息的可能性。

- producer(生产者)缓存每个分区未发送消息。缓存的大小是通过 batch.size 配置指定的。值较大的话将会产生更大的批。并需要更多的内存（因为每个“活跃”的分区都有1个缓冲区）。

- 默认缓冲可立即发送，即遍缓冲空间还没有满，但是，如果你想减少请求的数量，可以设置linger.ms大于0。这将指示生产者发送请求之前等待一段时间，希望更多的消息填补到未满的批中。这类似于TCP的算法，例如上面的代码段，可能100条消息在一个请求发送，因为我们设置了linger(逗留)时间为1毫秒，然后，如果我们没有填满缓冲区，这个设置将增加1毫秒的延迟请求以等待更多的消息。需要注意的是，在高负载下，相近的时间一般也会组成批，即使是 linger.ms=0。在不处于高负载的情况下，如果设置比0大，以少量的延迟代价换取更少的，更有效的请求。

- buffer.memory 控制生产者可用的缓存总量，如果消息发送速度比其传输到服务器的快，将会耗尽这个缓存空间。当缓存空间耗尽，其他发送调用将被阻塞，阻塞时间的阈值通过max.block.ms设定，之后它将抛出一个TimeoutException。

- key.serializer和value.serializer示例，将用户提供的key和value对象ProducerRecord转换成字节，你可以使用附带的ByteArraySerializaer或StringSerializer处理简单的string或byte类型。

## 消费者详解

上文中消费者代码

```
Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test");
        props.put("enable.auto.commit", "true");
        props.put("auto.commit.interval.ms", "1000");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("test2"));
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(100);
            for (ConsumerRecord<String, String> record : records)
                System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
        }
```

## 偏移量和消费者的位置

kafka为分区中的每条消息保存一个偏移量（offset），这个偏移量是该分区中一条消息的唯一标示符。也表示消费者在分区的位置。例如，一个位置是5的消费者(说明已经消费了0到4的消息)，下一个接收消息的偏移量为5的消息。实际上有两个与消费者相关的“位置”概念：

消费者的位置给出了下一条记录的偏移量。它比消费者在该分区中看到的最大偏移量要大一个。 它在每次消费者在调用poll(long)中接收消息时自动增长。

“已提交”的位置是已安全保存的最后偏移量，如果进程失败或重新启动时，消费者将恢复到这个偏移量。消费者可以选择定期自动提交偏移量，也可以选择通过调用commit API来手动的控制(如：commitSync 和 commitAsync)。上述代码就是自动提交偏移量。

这个区别是消费者来控制一条消息什么时候才被认为是已被消费的，控制权在消费者，下面我们进一步更详细地讨论。

#### 消费者组和主题订阅

Kafka的消费者组概念，通过进程池瓜分消费和处理消息的工作。这些进程可以在同一台机器运行，也可分布到多台机器上，增加可扩展性和容错性,相同group.id的消费者将视为同一个消费者组。

分组中的每个消费者通过subscribe API动态的订阅一个topic列表。kafka将已订阅topic的消息发送到每个消费者组中。并通过平衡分区在消费者分组中所有成员之间来达到平均。因此每个分区恰好地分配1个消费者（一个消费者组中）。所有如果一个topic有4个分区，并且一个消费者分组有2个消费者。那么每个消费者消费2个分区。

消费者组的成员是动态维护的：如果一个消费者故障。分配给它的分区将重新分配给同一个分组中其他的消费者。同样的，如果一个新的消费者加入到分组，将从现有消费者中移一个给它。这被称为重新平衡分组，并在下面更详细地讨论。 当新分区添加到订阅的topic时，或者当创建与订阅的正则表达式匹配的新topic时，也将重新平衡。将通过定时刷新自动发现新的分区，并将其分配给分组的成员。

从概念上讲，你可以将消费者分组看作是由多个进程组成的单一逻辑订阅者。作为一个多订阅系统，Kafka支持对于给定topic任何数量的消费者组，而不重复。

这是在消息系统中常见的功能的略微概括。所有进程都将是单个消费者分组的一部分（类似传统消息传递系统中的队列的语义），因此消息传递就像队列一样，在组中平衡。与传统的消息系统不同的是，虽然，你可以有多个这样的组。但每个进程都有自己的消费者组（类似于传统消息系统中pub-sub的语义），因此每个进程都会订阅到该主题的所有消息。

此外，当分组重新分配自动发生时，可以通过ConsumerRebalanceListener通知消费者，这允许他们完成必要的应用程序级逻辑，例如状态清除，手动偏移提交等。有关更多详细信息，请参阅[Kafka存储的偏移](https://link.zhihu.com/?target=http%3A//kafka.apache.org/0101/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html%23rebalancecallback)。

它也允许消费者通过使用assign(Collection)手动分配指定分区，如果使用手动指定分配分区，那么动态分区分配和协调消费者组将失效。

## 发现消费者故障

订阅一组topic后，当调用poll(long）时，消费者将自动加入到组中。只要持续的调用poll，消费者将一直保持可用，并继续从分配的分区中接收消息。此外，消费者向服务器定时发送心跳。 如果消费者崩溃或无法在session.timeout.ms配置的时间内发送心跳，则消费者将被视为死亡，并且其分区将被重新分配。

还有一种可能，消费可能遇到“活锁”的情况，它持续的发送心跳，但是没有处理。为了预防消费者在这种情况下一直持有分区，我们使用max.poll.interval.ms活跃检测机制。 在此基础上，如果你调用的poll的频率大于最大间隔，则客户端将主动地离开组，以便其他消费者接管该分区。 发生这种情况时，你会看到offset提交失败（调用commitSync（）引发的CommitFailedException）。这是一种安全机制，保障只有活动成员能够提交offset。所以要留在组中，你必须持续调用poll。

消费者提供两个配置设置来控制poll循环：

1. max.poll.interval.ms：增大poll的间隔，可以为消费者提供更多的时间去处理返回的消息（调用poll(long)返回的消息，通常返回的消息都是一批）。缺点是此值越大将会延迟组重新平衡。
2. max.poll.records：此设置限制每次调用poll返回的消息数，这样可以更容易的预测每次poll间隔要处理的最大值。通过调整此值，可以减少poll间隔，减少重新平衡分组的

对于消息处理时间不可预测地的情况，这些选项是不够的。 处理这种情况的推荐方法是将消息处理移到另一个线程中，让消费者继续调用poll。 但是必须注意确保已提交的offset不超过实际位置。另外，你必须禁用自动提交，并只有在线程完成处理后才为记录手动提交偏移量（取决于你）。 还要注意，你需要pause暂停分区，不会从poll接收到新消息，让线程处理完之前返回的消息（如果你的处理能力比拉取消息的慢，那创建新线程将导致你机器内存溢出）。

## 控制消费的位置

大多数情况下，消费者只是简单的从头到尾的消费消息，周期性的提交位置（自动或手动）。kafka也支持消费者去手动的控制消费的位置，可以消费之前的消息也可以跳过最近的消息。

有几种情况，手动控制消费者的位置可能是有用的。

一种场景是对于时间敏感的消费者处理程序，对足够落后的消费者，直接跳过，从最近的消费开始消费。

另一个使用场景是本地状态存储系统。在这样的系统中，消费者将要在启动时初始化它的位置（无论本地存储是否包含）。同样，如果本地状态已被破坏（假设因为磁盘丢失），则可以通过重新消费所有数据并重新创建状态（假设kafka保留了足够的历史）在新的机器上重新创建。

kafka使用seek(TopicPartition, long)指定新的消费位置。用于查找服务器保留的最早和最新的offset的特殊的方法也可用（seekToBeginning(Collection) 和 seekToEnd(Collection)）。

## 消费者流量控制

如果消费者分配了多个分区，并同时消费所有的分区，这些分区具有相同的优先级。在一些情况下，消费者需要首先消费一些指定的分区，当指定的分区有少量或者已经没有可消费的数据时，则开始消费其他分区。

例如流处理，当处理器从2个topic获取消息并把这两个topic的消息合并，当其中一个topic长时间落后另一个，则暂停消费，以便落后的赶上来。

kafka支持动态控制消费流量，分别在future的poll(long)中使用pause(Collection) 和 resume(Collection) 来暂停消费指定分配的分区，重新开始消费指定暂停的分区。

# 常见web攻防简易总结

## XSS攻击

即跨站脚本攻击（利用站内的信任用户）

如：url后参数带js脚本，提交表单中带js脚本到数据库中

防范：进行html转义，

如 <变成&lt; >变成&gt; '变成&amp;

jstl自带html转义，escapeMxl="true"

防止js脚本读取cookie信息：将cookie设置成httponly

## CRSF攻击

跨站请求伪造 （通过伪装受信任用户的请求）

原理：用户C访问安全站点A，A返回C一个cookie，C没退出A就访问恶意站点B，B让C访问A，此时带有cookie。A不能判断这个是恶意站点的请求，从而让B的请求得逞

原理图如下：

![img](https://pic2.zhimg.com/80/v2-a061eafcda3ae6e8b577b5e29299eda9_hd.png)

如：A网站转账接口：transfer.do?num=10001&money=1000

某大型论坛B，一个恶意用户上传了图片，但是地址改成了transfer.do?num=10001&money=1000。用户登录网站A后没有及时登出，访问了B网站，就会少1000块

当然现在没人用get转账，都用post。

但要是恰好B网站存在XSS漏洞，把form表单插在B网站内，同样会产生CRSF攻击

防范：

- 1.将cookie设置成httponly （通过js脚本将无法读取到cookie信息，这样能有效的防止XSS攻击）
- 2.增加token
- 3.通过referer识别（HTTP头中的字段，记录了该HTTP请求的来源地址）

## SQL注入攻击

把SQL命令伪装成正常的HTTP请求参数，传递到服务端

如：提交密码： ' or '1'='1 或';drop table aaa-- (--注释后面的语句)

防范：

- 1.使用预编译语句 PreparedStatement 创建PreparedStatement对象时就发送给DBMS(数据库管理系统)进行编译，会通过参数占位符来代替参数，会把特殊字符进行转义
- 2.使用orm框架
- 3.对密码加密

## DDoS攻击

即分布式拒绝服务攻击

原理：通过大量计算机同时对某一主机进行访问，使主机瘫痪。

如：

- 1.SYN Flood

因TCP三次握手中服务器得等待客户端传回ack报文，但SYN Flood伪造了大量不存在的IP进行TCP连接，使服务器瘫痪。

- 2.DNS Query Flood

跟SYN Flood差不多，是向被攻击服务器发送海量的域名解析请求。因为这些域名都是随机生成的，在本地没有缓存，就会向上层服务器递归查询，直到全球互联网的13台根dns服务器。当解析请求超过一定量时，就会造成解析超时。

- [http://3.CC](https://link.zhihu.com/?target=http%3A//3.CC)攻击

原理：通过大量计算机进行匿名HTTP代理，模拟正常用户给网站发起请求，直到拖垮后端系统，无法为正常用户提供服务。

# 分布式网站优化简易总结

## web前端性能优化

**浏览器访问优化：**

- 1.减少HTTP请求数：合并CSS、合并JS、合并图片
- 2.使用浏览器缓存：设置HTTP头中的Cache-Control和Expires属性，缓存CSS、Js、图标到浏览器。
- 3.启动压缩：把HTML、CSS、JS文件启用GZip压缩
- 4.CSS放在页面最上面、JS放在页面最下面：为了让浏览器尽快加载CSS，而在稍后执行JS
- 5.减少cookie传输

**CDN加速：**

CDN本质是一个缓存，而且将静态数据缓存在离用户最近的地方，所以可以使用户最快获取数据。

如图所示：

![img](https://pic1.zhimg.com/80/v2-73233fb165b330b36f8b9e9c9d91a4d4_hd.png)

**反向代理：**

代理服务器在应用服务器之前，也能缓存静态文件或热门信息。也可作为负载均衡和一层安全防护。

原理如图：

![img](https://pic1.zhimg.com/80/v2-9c26f4fc157624d41acfc70dd13580bc_hd.png)

## 应用服务器性能优化

**分布式缓存：**

放很少变化的数据，读不到再从数据库中读。

如图：

![img](https://pic3.zhimg.com/80/v2-74b761c3987812de5a077b34b86c5cde_hd.png)

实现：JBoss Cache，Memcached，Redis

**异步操作：**

使用消息队列。（如新浪微博发消息）

如图

![img](https://pic4.zhimg.com/80/v2-8ff329c96da97631d2c859ad0de1b85f_hd.png)

有句话说的好：任何可以晚点做的事情都应该晚点再做。

**使用集群：(负载均衡)**

如图

![img](https://pic3.zhimg.com/80/v2-3aaf3632f35cf04eb39ce7816663d7c2_hd.png)

**代码优化：**

- 1.多线程
- 2.资源复用：单例，对象池
- 3.数据结构
- 4.垃圾回收：JVM

## 存储性能优化：

- 1.固态硬盘
- 2.LSM树
- 3.HDFS



























































































