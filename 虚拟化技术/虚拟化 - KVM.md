# 虚拟化 - KVM

## 云计算模式

### 软件即服务(SaaS)

在 SaaS模式中，厂商将应用软件统一部署在自己的服务器上，客户可以根据自己实际需求，通过互联网向厂商定购所需的应用软件服务，按定购的服务多少和时间 长短向厂商支付费用，并通过互联网获得厂商提供的服务。

### 平台即服务(PaaS)

通过PaaS这种模式，用户可以在一个包括SDK，文档和测试环境等在内的开发平台上非常方便地编写应用，而且不论是在部署，或者在运行的时候，用户都无需 为服务器，操作系统，网络和存储等资源的管理操心。

### 基础设施即服务(IaaS)

通过IaaS这种模式，用户可以从供应商那里获得他所需要的虚拟机或者存储等资源来装载相关的应用，同时这些基础设施的繁琐的管理工作将由IaaS供应商来处理。

## 虚拟化技术

虚拟化及时是一种资源管理(优化)技术。根据不用对象类型，可以细分为：

- **平台虚拟化（Platform Virtualization）：**针对计算机和操作系统的虚拟化。
- **资源虚拟化（Resource Virtualization）：**针对特定的系统资源的虚拟化，如内存、存储、网络资源等。
- **应用程序虚拟化（Application Virtualization）：**包括仿真、模拟、解释技术等，如 Java 虚拟机（JVM）。

### 平台虚拟化

VMM 运行在传统的操作系统上，就像其他计算机程序那样运行。

![img](https://ask.qcloudimg.com/http-save/yehe-4895051/0net8avhv6.jpeg?imageView2/2/w/1620)

### 全虚拟化

全虚拟化是指虚拟机模拟了完整的底层硬件，包括处理器、物理内存、时钟、外设等，使得为原始硬件设计的操作系统或其它系统软件完全不做任何修改就可以在虚拟机中运行。

一般的，CPU 都会划分为用户态和内核态，而 x86 CPU 更是细分为了 Ring 0~3 四种执行状态。

Ring0 核心态（Kernel Mode）：是操作系统内核的执行状态（运行模式），运行在核心态的代码可以无限制的对系统内存、设备驱动程序、网卡接口、显卡接口等外部设备进行访问。

Ring3 用户态（User Mode）：运行在用户态的程序代码，需要受到 CPU 的检查，受限的内存访问。

![img](https://ask.qcloudimg.com/http-save/yehe-4895051/27chw7gl5z.jpeg?imageView2/2/w/1620)

工作原理：全虚拟化需要在 VMM 中模拟出一颗包含了控制单元、运算单元、存储单元、IS（指令集）的 CPU；此外，还需要模拟一张进行虚拟存储地址和物理存储地址转换的页表；此外，还需要在 VMM 模拟磁盘设备控制器、网络适配器等等各种 I/O 外设接口。如此依赖，Guest OS 就不知道自己其实是个虚拟机了呀，它受到了欺骗。

### 半虚拟化

半虚拟化是一种通过修改 Guest OS 部分访问特权状态的代码以便直接与 VMM 交互的技术。部分硬件接口以软件的形式提供给 Guest OS，这可以通过 Hypercall（VMM 提供给 Guest OS 的直接调用，与系统调用类似）的方式来提供。

![img](https://ask.qcloudimg.com/http-save/yehe-4895051/6d8j7581ts.jpeg?imageView2/2/w/1620)

相较于全虚拟化，半虚拟化 VMM 只需要模拟部分底层硬件，因此 Guest OS 不做修改是无法在虚拟机中运行的，甚至运行在虚拟机中的其它程序也需要进行修改，如此代价，换来的就是接近于物理机的虚拟机性能。

### 硬件辅助全虚拟化

![img](https://ask.qcloudimg.com/http-save/yehe-4895051/4c7izurj7y.jpeg?imageView2/2/w/1620)

- Intel-VT（Intel Virtualization Technology）
- AMD-V 

### 操作系统虚拟化(容器)

操作系统虚拟化（OS-level virtualization） 是一种在服务器操作系统中使用的、没有 VMM 层的轻量级虚拟化技术，内核通过创建多个虚拟的操作系统实例（内核和库）来隔离不同的进程（容器），不同实例中的进程完全不了解对方的存在。![img](https://ask.qcloudimg.com/http-save/yehe-4895051/rqoucj9zht.jpeg?imageView2/2/w/1620)

## KVM

kvm是一种用于Linux内核中的虚拟化基础设施，可以将Linux内核转化为一个hypervisor。包含一个为处理器提供底层虚拟化 可加载的核心模块kvm.ko（kvm-intel.ko或kvm-AMD.ko）还需要一个经过修改的QEMU软件（qemu-kvm），作为虚拟机上层控制和界面。

### 功能特性

- 内存管理：kvm在支持Intel的扩展页表(EPT)和AMD的嵌套页表(NPT)，内存共享通过KSM的内核功能来支持。
- 存储：KVM能够使用Linux支持的任何存储来存储虚拟机镜像。包括IDE,SCS和SATA的本地磁盘，NAS，iSCSI和光纤通信。
- 设备驱动程序：KVM支持混合虚拟机，允许虚拟机使用优化的I/O接口而不使用模拟的设备，从而为网络和块设备提供高性能的I/O。
- 性能和可伸缩性：继承来Linux的性能和可伸缩性。

### KVM架构

![img](https://ask.qcloudimg.com/http-save/yehe-1043708/rvd3d5f8k8.png?imageView2/2/w/1620)

- **/dev/kvm** 接口是 Qemu 和 KVM 交互的“桥梁”


- 基本的原理：/dev/kvm 本身是一个设备文件，这就意味着可以通过 ioctl 函数来对该文件进行控制和管理，从而可以完成用户空间与内核空间的数据交互。在 KVM 与 Qemu 的通信过程主要就是一系列针对该设备文件的 ioctl 调用。

- KVM模块：是KVM虚拟机的核心部分，主要功能是初始化CPU硬件，打开虚拟模式，然后将虚拟客户机运行在虚拟机模式下，并对虚拟客户机的运行提供一定的支持。

  - 初始化：先初始化内部数据结构；检测当前CPU，打开控制寄存器CR4中的虚拟模式开关；执行VMXON指令，将宿主操作系统至于虚拟化模式的根模式；KVM模块创建特殊设备文件/dev/kvm/并等待来自用户控件的命令
  - KVM与QEMU通信接口：主要是一系列针对特殊设备的IOCTL调用。可以对虚拟机做相应的管理，比如创建用户控件虚拟地址，物理地址的映射关系；创建多个可供运行的虚拟处理器(vcpu)。

- 内存虚拟化

  处理器中的内存管理单元(MMU)是通过页表的形式将程序运行的虚拟地址转换成物理内存地址。在虚拟机模式下，内存管理单元的页表必须在一次查询的时候完成两次地址转换。这是因为，除了要将客户机程序的虚拟地址转换成为客户机物理内存地址，还必须将客户机物理地址转换成真实物理地址。

  因此做了EPT技术，通过引入第二级页表来描述客户机虚拟地址和真实物理地址转换，硬件可以自动进行两级转换生成正确的内存访问地址。



### QEMU

Qemu 是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备，我们最熟悉的就是能够模拟一台能够独立运行操作系统的虚拟机，虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件。

![img](https://ask.qcloudimg.com/http-save/yehe-1043708/oz83fzquuj.png?imageView2/2/w/1620)

从本质上看，虚拟出的每个虚拟机对应 host 上的一个 Qemu 进程，而虚拟机的执行线程（如 CPU 线程、I/O 线程等）对应 Qemu 进程的一个线程。

## KVM核心

### CPU虚拟化

QEMU/KVM中，QEMU提供对CPU的模拟，展现一定的CPU数目和CPU的特性；在KVM打开的情况下，客户机中CPU指令的执行由硬件处理器的虚拟化功能来辅助执行。

#### VCPU

在KVM环境中，每个客户机都是一个标准的Linux进程(QEMU进程)，而每一个vcpu是QEMU派生出的一个普通线程。

vcpu执行模式：

1. 用户模式：主要处理I/O的模拟和管理，由QEMU的代码实现；
2. 内核模式：处理需要高性能和安全相关的指令，如客户模式到内核模式的转换，处理器模式下的I/O，影子内存管理；
3. 客户模式：执行guest的大部分指令，I/O和一些特权指令除外；

![vCPU在KVM中的三种执行模式](https://img-blog.csdn.net/20150510150314072)

#### SMP

SMP技术就是对称多处理结构，这种结构的最大特点就是CPU共享所有资源，比如总线，内存，IO系统等等。缺点是扩展性有限，支持的CPU数量个数有限。

![img](https://ask.qcloudimg.com/http-save/yehe-3636591/gg51emxmb8.png?imageView2/2/w/1620)

#### NUMA

NUMA模式，每个CPU都有雨自己的存储器，每个处理器也可以访问其他CPU的存储器。访问自己的存储器要比访问别的存储器快很多。

![img](https://ask.qcloudimg.com/http-save/yehe-3636591/tmsyzpq6jk.png?imageView2/2/w/1620)

#### CPU过载

KVM允许客户机过载使用物理资源，分配的CPU和内存数量多于物理实际的资源。QEMU会启动更多的线程来为客户机提供服务，这些线程也是被Linux内核调度运行在物理CPU硬件上。

最推荐的做法是对多个单CPU的客户机使用over-commit，比如：在拥有4个逻辑CPU的宿主机中，同时运行多于4个（如8个、16个）客户机，其中每个客户机都被分配一个vCPU。这时，如果每个宿主机的负载不很大的情况下，宿主机Linux对每个客户机的调度是非常有效的，这样的过载使用并不会带来客户机中的性能损失。

最不推荐的做法是让某一个客户机的vCPU数量超过物理CPU数量。

#### 亲和性

进程的处理器亲和性，即是CPU的绑定设置，是指将进程绑定到特定的一个或多个CPU上去执行，而不允许调度到其他的CPU上。好处是可以减少进程在多个CPU之间交换运行带来的缓存命中失效，带来一定程度上的性能提升。但是破坏各个CPU的负载均衡。

每个虚拟机的vcpu在宿主机上，都表现为一个普通的qemu线程，可以使用taskset工具对其进行设置处理亲和性，使其绑定到某一个或几个固定的cpu上去调度。尽管Linux内核的进程调度算法已经非常高效了，在多数情况下是不需要对进程的调度进行干预，不过，在虚拟化环境中，有时有必要将客户机的qemu进程或线程绑定到固定的cpu上执行。

### 内存虚拟化

通过内存虚拟化共享物理系统内存，动态分配给虚拟机。

KVM 实现客户机内存的方式是，利用mmap系统调用，在QEMU主线程的虚拟地址空间中申明一段连续的大小的空间用于客户机物理内存映射。

在有两个虚机的情况下，情形是这样的：

![img](https://ask.qcloudimg.com/http-save/yehe-1220175/p19q9pgt21.jpeg?imageView2/2/w/1620)

KVM 为了在一台机器上运行多个虚拟机，需要增加一个新的内存虚拟化层，也就是说，必须虚拟 MMU 来支持客户操作系统，来实现 VA -> PA -> MA 的翻译。

#### EPT

EPT(扩展页表)，作为CPU中新的一层，用来将客户机的物理地址翻译为主机的物理地址。![img](https://ask.qcloudimg.com/http-save/yehe-1220175/kup99hdvof.jpeg?imageView2/2/w/1620)

它的两阶段记忆体转换，特点就是将 Guest Physical Address → System Physical Address

#### 影子页表

KVM需要为每个客户机的每个进程的页表都要维护一套相应的影子页表， 这会带来较大内存上的额外开销，此外，客户机页表和和影子页表的同步也比较复杂。

#### ![img](https://ask.qcloudimg.com/http-save/yehe-1220175/9y2k0t3pt.jpeg?imageView2/2/w/1620)

#### KSM

KSM 作为内核中的守护进程（称为 ksmd）存在，它定期执行页面扫描，识别副本页面并合并副本，释放这些页面以供它用。因此，在多个进程中，Linux将内核相似的内存页合并成一个内存页。减少多个相似的虚拟机内存占用，提高内存的使用效率。

合并过程

- 初始状态

![img](https://ask.qcloudimg.com/http-save/yehe-1220175/ceklctyv2m.jpeg?imageView2/2/w/1620)

- 合并后

  ![img](https://ask.qcloudimg.com/http-save/yehe-1220175/or5sqz41vz.jpeg?imageView2/2/w/1620)

- Guest 1 写内存后

  ![img](https://ask.qcloudimg.com/http-save/yehe-1220175/pjdceucrad.jpeg?imageView2/2/w/1620)

#### Huge Page

 HugePage，就是指的大页内存管理方式。与传统的4kb的普通页管理方式相比，HugePage为管理大内存(8GB以上)更为高效。

- Regular Pages

图中物理地址page size大小4kb。也可以看到进程1和进程2在system-wide table中都指向了page2，也就是同一个物理地址。

![img](https://ask.qcloudimg.com/http-save/yehe-2746053/u6l5orbntb.jpeg?imageView2/2/w/1620)

- Huge Pages

  page table中的任意一个page可能使用了常规的page，
  也有可能使用了huge page。同样进程1和进程2都共享了其中的Hpage2。图中的物理内存常规的page size是4kb，huge page size 是4mb。

  ![img](https://ask.qcloudimg.com/http-save/yehe-2746053/3dqn40zdgm.jpeg?imageView2/2/w/1620)

#### 内存过载

有如下三种方式来实现内存的过载使用

- 内存交换：用交换空间来弥补内存的不足
- 内存气球：通过virio_ballon驱动来实现宿主机和客户机的协作
- 页共享：通过KSM合并多个客户机进程使用的相同内存页

### 存储配置

QEMU提供了对多块存储设备的模拟，包括IDE，SCSI，软盘，U盘，virtio磁盘等。而且对设备的启动顺序提供了灵活的配置。

#### QEMU支持镜像

qemu-img支持非常多种的文件格式，包括：cow,ftps,https,http,dmg,qcow,qcow2,raw,sheepdog,vdi,vpc等

qcow2:是QEMU目前推荐的镜像格式，支持稀疏文件以节省存储空间，支持可选的AES加密以提高镜像文件安全性，支持基于zlib的压缩，支持在一个镜像文件中有多个虚拟机快照。

#### 镜像存储方式

客户机镜像存储文件，其中几种：

- 本地存储的客户机镜像文件
- 物理磁盘或磁盘分区
- LVM，逻辑分区
- NFS，网络文件系统
- iSCSI，基于Internet的小型计算机系统接口
- 本地或光纤通道连接的LUN
- GFS2

### 网络配置

#### QEMU支持的网络模式

- 基于网桥的虚拟网卡：可以让客户机和宿主机共享一个物理网络设备连接网络，客户机有独立的IP
- 基于NAT的虚拟网络：将内网地址转换为外网的合法IP地址，被广泛应用于各种类型的Internet接入方式
- QEMU内置的用户模式网络：完全由QEMU自身实现，使用Slirp实现一整套TCP/IP协议栈，并使用这个协议栈实现一套虚拟的NAT网络
- 直接分配网络设备的网络(VT-d,SR-IOV)

### 图形显示

#### SDL

提供一个简单的接口用于操作硬件平台的图形界面，声音，输入设备等。在QEMU模拟器中的图形显示默认就是使用SDL。

#### VNC

VNC是图形化的桌面分享系统，使用RFB协议来远程控制另外一台计算机系统。通过网络将控制端的键盘，鼠标操作传递到远程受控计算机中，而将远程计算机中的图形显示屏幕反向传输回控制端的VNC窗口中。

## KVM高级

### 半虚拟化驱动

#### virtio

是一种 I/O 半虚拟化解决方案，是一套通用 I/O 设备虚拟化的程序，是对半虚拟化 Hypervisor 中的一组通用 I/O 设备的抽象。提供了一套上层应用与各 Hypervisor 虚拟化设备（KVM，Xen，VMware等）之间的通信框架和编程接口，减少跨平台所带来的兼容性问题，大大提高驱动程序开发效率。![img](https://ask.qcloudimg.com/http-save/yehe-1043708/gxii9q5n25.png?imageView2/2/w/1620)

半虚拟化通过底层硬件辅助的方式，将部分没必要虚拟化的指令通过硬件来完成，Hypervisor 只负责完成部分指令的虚拟化

由于不同 guest 前端设备其工作逻辑大同小异（如块设备、网络设备、PCI设备、balloon驱动等），单独为每个设备定义一套接口实属没有必要，而且还要考虑扩平台的兼容性问题，另外，不同后端 Hypervisor 的实现方式也大同小异（如KVM、Xen等），这个时候，就需要一套通用框架和标准接口（协议）来完成两者之间的交互过程，virtio 就是这样一套标准，它极大地解决了这些不通用的问题。

![img](https://ask.qcloudimg.com/http-save/yehe-1043708/xdaox4v4mm.png?imageView2/2/w/1620)

#### virtio_balloon

通常情况，要改变客户机占用的宿主机内存，要先关闭客户机，修改启动时的内存配置，然后重启客户机。而内存ballooning技术可以在客户机运行时动态跳帧它所占用的宿主机内存资源。

该技术能：

- 当宿主机内存紧张时，可以请求客户机回收利用已分配给客户机的部分内存，客户机就会释放部分空闲内存。若其内存空间不足，可能还会回收部分使用中的内存，可能会将部分内存换到交换分区中。
- 当客户机内存不足时，也可以让客户机的内存气球压缩，释放出内存气球中的部分内存，让客户机使用更多的内存。

![img](https://ask.qcloudimg.com/http-save/yehe-1220175/xjzfdxjswc.jpeg?imageView2/2/w/1620)

原理：

1. KVM 发送请求给 VM 让其归还一定数量的内存给KVM。
2. VM 的 virtio_balloon 驱动接到该请求。
3. VM 的驱动是客户机的内存气球膨胀，气球中的内存就不能被客户机使用。
4. VM 的操作系统归还气球中的内存给VMM
5. KVM 可以将得到的内存分配到任何需要的地方。
6. KVM 也可以将内存返还到客户机中。

优势和不足：

| 优势                                       | 不足                                       |
| :--------------------------------------- | :--------------------------------------- |
| 1. ballooning 可以被控制和监控				          2.对内存的调节很灵活，可多可少。				          3.KVM 可以归还内存给客户机，从而缓解其内存压力。 | 1.需要客户机安装驱动								                                         2.大量内存被回收时，会降低客户机的性能。                                                                      3.目前没有方便的自动化的机制来管理 ballooning，一般都在 QEMU 的 monitor 中执行命令来实现。				                                                                                                 4.内存的动态增加或者减少，可能是内存被过度碎片化，从而降低内存使用性能。 |

#### virtio_net

e1000/rtl8139类型的网卡，CPU会把很多时间花费在写register，读register上。而且Guest在写register的时候，会让vm陷入异常，退出VM mode。

virtio场景下的数据流向，就会变得短一些了：虚拟机中发送的packet，放到了虚拟机的内存（VringBuf）中。然后继续使用PIO访问，那么，QEMU就会截获到PIO操作。然后QEMU计算出来 VringBuf的HVA，把数据再写到TAP网卡中。差别就在于PIO做控制用，只需要传递少量的数据，大部分的数据通过共享内存实现。

![img](https://ask.qcloudimg.com/http-save/yehe-154642/hum75ufe1j.png?imageView2/2/w/1620)

#### virtio_blk

#### kvm_clock

### 设备直接分配

#### VT-d

客户机使用的设备：

- Emulated device：QEMU纯软件模拟的设备
- Virtio device：实现VIRTIO API的半虚拟化驱动的设备
- PCI device assignment：PCI 设备直接分配

模拟I/O设备方式的优点是对硬件平台依赖性较低，不需要宿主机和客户机的额外支持。而缺点是I/O路劲较长，VM-Exit次数很多，因此性能较差。

virtio半虚拟化设备实现了VIRTIO API，减少了VM-Exit，提高用户机I/O效率

PCI设备直接分配，它允许将宿主机的物理PCI设备直接分配给客户机完全使用。KVM虚拟机支持将宿主机中的PCI，PCI-E设备附加到虚拟化的客户机中，从而让客户机以独占方式访问这个PCI设备。

#### SR-IOV

尽管VT-d技术实现了设备直接分配，性能非常好，但它的一个物理设备只能分配一个客户机使用。为了实现多个虚拟机能够共享同一个物理设备的资源，并且达到设备直接分配的性能，SR-IOV定义了一个标准化的机制用以原生的支持是吸纳多个共享设备。

SR-IOV引入了两个PCIe的完整功能

- PF 物理功能：包括管理SR-IOV功能在内的所有PCIe 功能。
- VF 虚拟功能：一部分轻量级的PCI-e 功能，只能进行必要的数据操作和配置。

通过为虚拟机提供独立的内存地址、中断和DMA流而避免VMM的介入。

### 设备热插拔

实现热插拔需要：

- 总线电气特性
- 主板BIOS
- 操作系统
- 设备驱动

可以实现热插拔的部件主要是SATA硬盘，CPU，内存，风扇，USB，网卡等

### 动态迁移

迁移包括系统整体的迁移和某个工作负载的迁移。系统整体迁移是将系统上的所有软件完全复制到另一台物理硬件机器上。而工作负载迁移，是将系统上的某个工作负载转移到另一台物理硬件机器上。

在虚拟化环境中的迁移，又分为静态迁移和动态迁移。静态迁移有明显一段时间客户机的服务不可用，而动态迁移则没有明显的服务暂停时间。

静态迁移也可以分为两种，一种是关闭客户机后，将其硬盘镜像复制到另一台宿主机上然后恢复启动，这种迁移不能保留客户机中运行的工作负载；另一种是两台宿主机共享存储系统，只需要在暂停客户机后，复制其内存镜像到另一台宿主机中恢复启动。

#### 迁移效率和应用

从如下几个方面来衡量虚拟机迁移的效率

1. 整体迁移时间：从源主机中迁移操作开始到目的主机上客户机服务处于不可用状态的时间，此时源主机上客户机已经暂停服务，目的主机上的客户机还未恢复服务。
2. 服务器停机时间：在迁移过程中，源主机和目的主机上的客户机都处于不可用状态的时间，此时源主机上客户机已暂停，目的目的主机上客户还未恢复服务。
3. 对服务的性能影响：不仅包括迁移后的客户机中应用程序的性能与迁移前相对比是否有所降低，还包括迁移后对目的主机上的其他服务的性能影响。

动态迁移的整体迁移时间受：hypervisor和迁移工具的种类，磁盘存储的大小，内存大小及使用率，CPU性能及使用率，网络带宽大小及是否拥塞等。

应用场景：

1. 负载均衡：当一台为服务器的负载较高时，可以将其上运行的客户机动态迁移到负载低的主机
2. 接触硬件依赖：当系统管理员需要在宿主机上添加硬件设备，可以把宿主机的应用暂时迁移到其他的客户机上，这样用户就感觉不到服务有任何暂停的问题
3. 节约资源：当几台客户机的负载都较低的情况下，可以把应用都暂时迁移到一台客户机上，关闭不用的客户机，从而节省电力
4. 可以实现客户机的远程迁移

#### KVM动态迁移

在基于**共享存储系统**时，KVM 动态迁移的具体过程为：

1. 迁移开始时，客户机依然在宿主机上运行，与此同时，客户机的内存页被传输到目的主机上。
2. QEMU/KVM 会监控并记录下迁移过程中所有已被传输的内存页的任何修改，并在所有内存页都传输完成后即开始传输在前面过程中内存页的更改内容。
3. QEMU/KVM 会估计迁移过程中的传输速度，当剩余的内存数据量能够在一个可以设定的时间周期（默认 30 毫秒）内传输完成时，QEMU/KVM会关闭源宿主机上的客户机，再将剩余的数据量传输到目的主机上，最后传输过来的内存内容在目的宿主机上恢复客户机的运行状态。
4. 至此，KVM 的动态迁移操作就完成了。迁移后的客户机尽可能与迁移前一致，除非目的主机上缺少一些配置，比如网桥等。

注意，当客户机中内存使用率非常大而且修改频繁时，内存中数据不断被修改的速度大于KVM能够传输的内存速度时，动态迁移的过程是完成不了的，这时候只能静态迁移。

在KVM中，动态迁移服务停机时间会与实际的工作负载和网络带宽等诸多因素有关，当网络代管过小或网络拥塞，会导致服务停机时间边长。

**迁移注意事项：**

1. 最好迁移的服务器cpu品牌一样
2. 64位只能在64位宿主机间迁移，32位可以迁移32位和64位宿主机
3. 宿主机名字不能冲突
4. 目的宿主机和源宿主机软件配置尽可能的相同，如 有相同的桥接网卡，资源池等。
5. 两台迁移的主机 cat/proc/cpuinfo |grep nx 的设置是相同的

### 嵌套虚拟化

指在虚拟化的客户机中运行一个Hypervisor，从而再虚拟化一个客户机。嵌套虚拟化不仅可以包含相同的Hypervisor嵌套，也可以包括不同Hypervisor的相互嵌套。

应用场景：

1. IaaS类型的云计算提供商 ：如果有了嵌套虚拟化功能的支持，就可以为其客户提供让客户可以自己运行所需Hypervisor和客户机的能力。
2. 为测试和调试Hypervisor带来了非常大的便利：了嵌套虚拟化的支持，被调试Hypervisor运行在更底层的Hypervisor之上，遇到被调试Hypervisor的系统崩溃，也只需要在底层的Hypervisor上重启被调试系统即可。
3. 在一些为了起到安全作用的带有Hypervisor的固件上：如果有嵌套虚拟化的支持，则在它上面不仅可以运行一些普通的负载，还可以运行一些Hypervisor启动另外的客户机。
4. 嵌套虚拟化的支持，对虚拟机系统的动态迁移也提供了新的功能：从而可将一个Hypervisor及其上面运行的客户机作为单一的节点进行动态迁移，这对服务器的负载均衡及灾难恢复等有积极意义。
5. 嵌套虚拟化的支持，对于系统隔离性、安全性方面也提供更多的实施方案。

### KVM安全

1. SMEP（supervision Mode Execution Protection）监督模式执行保护：属于硬件CPU特性，让处于管理模式的程序不能执行用户模式下可以访问的内存空间的指令。在运行管理模式的CPU不能执行用户模式的内存页，可以较好的阻止渗透攻击。而且由于SMEP是CPU的一个硬件特性，这种安全保护非常方便和高效，带来的性能开销几乎可以忽略。

2. cgroups: 控制客户机的资源使用
   cgroups控制组是系统内核的特性，用于限制，记录和隔离进程组对物理资源的使用。cgroups提供了如下的功能：

   - 资源限制
   - 优先级控制:不同的进程组可以有不同的优先级
   - 记录: 记录每个进程组实际占用的资源数量
   - 隔离： 不同的进程组使用不同的命名空间
   - 控制： 控制进程的暂停，添加检查点，重启等
     cgoups 中有几个重要的概念：
   - task 任务，一个任务就是linux中的进程或线程
   - control group 控制组： 以某种标准划分的一组任务。cgoups，资源的控制是以控制组为单位来实现的
   - 层级体系： 控制组被组织成一颗有层级关系的控制树。
   - 子系统：一个子系统就是一个资源控制器

3. SELinux

   使用SELinux是可以减轻恶意攻击，恶意软件带来的损失，并提供对机密性，完整性有较高要求的信息安全保障。SELinux是基于强制访问控制(MAC)策略。

   强制访问控制模式为每一个应用程序都提供一个虚拟沙箱，只允许应用程序执行它设计需要的且在安全策略中明确允许的任务，对每个应用程序只分配它正常工作所需的对应特权。

4. sVirt

   　sVirt 项目集成SELinux强制访问控制 (MAC) 安全和基于 Linux 的KVM虚拟化。

5. Hypervisior的可信启动TXT， Tboot

   TXT技术通过三个方面提供安全和信任

   - 将数字指纹软件保存在一个可信用平台模块的受保护区域，每次软件启动，都会检测并确保数字指纹吻合，从而判断是否存在风险
   - 能阻止其他应用程序，操作系统或硬件修改某个应用程序的专用内存区
   - 如果某个应用程序崩溃，TXT将清除它在内存中的数据和芯片缓存区，避免攻击软件嗅探其参与数据

   基于 Intel TXT技术简历的可信平台，主要由三个部分组成：

   - 安全模式指令扩展 SMX：引入与安全技术相关的指令，系统能够进入和退出安全隔离环境
   - 认证代码模块 AC：由芯片厂商提供，经过数字签名认证，与芯片完全绑定的一段认证代码，作用是检测后续安全启动环境的可信性
   - 度量过的安全启动环境 MLE：需要对即将启动内核进行检测，同时还需要保证这种检测过程是受保护的，以及它自身所运行的代码不会被篡改

6. 其他安全策略：

   - 镜像安全：qcow2格式的镜像支持加密
   - 虚拟网络的安全




























