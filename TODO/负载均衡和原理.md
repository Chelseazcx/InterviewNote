# 负载均衡和原理

开头先理解一下所谓的“均衡”

不能狭义地理解为分配给所有实际服务器一样多的工作量，因为多台服务器的承载能力各不相同，这可能体现在硬件配置、网络带宽的差异，也可能因为某台服务器身兼多职，我们所说的“均衡”，也就是希望所有服务器都不要过载，并且能够最大程序地发挥作用。

## 解决的问题

1. 解决并发压力，提高应用处理性能（增加吞吐量，加强网络处理能力）；
2. 提供故障转移，实现高可用；
3. 通过添加或减少服务器数量，提供网站伸缩性（扩展性）；
4. 安全防护；（负载均衡设备上做一些过滤，黑白名单等处理）

原理 : 其实就是根据一些转发算法，讲请求分发到不同的节点上去执行

## 负载均衡原理

#### DNS

通过使用域名解析实现负载均衡，配置多个A 记录，这些A记录对应的服务器构成集群。大型网站总是部分使用DNS解析，作为第一级负载均衡。 显而易见，使用这种方式的负载均衡的控制权更在域名商那里，不易拓展，并且用这种方式的负载不能很好的分流，有可能造成所有的请求都集中到一个节点上，但是作为第一层的负载均衡的确是个好办法。

#### HTTP

当http代理（比如浏览器）向web服务器请求某个URL后，web服务器可以通过http响应头信息中的Location标记来返回一个新的URL。这意味着HTTP代理需要继续请求这个新的URL，完成自动跳转。

#### IP

在网络层通过修改请求目标地址进行负载均衡。 用户请求数据包，到达负载均衡服务器后，负载均衡服务器在操作系统内核进程获取网络数据包，根据负载均衡算法得到一台真实服务器地址，然后将请求目的地址修改为，获得的真实ip地址，不需要经过用户进程处理。 真实服务器处理完成后，响应数据包回到负载均衡服务器，负载均衡服务器，再将数据包源地址修改为自身的ip地址，发送给用户浏览器。

![示意图](https://user-gold-cdn.xitu.io/2016/11/29/431c62b29f9e25d9f5d28234b9edc06e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

#### 链路层

在通信协议的数据链路层修改mac地址，进行负载均衡。 数据分发时，不修改ip地址，指修改目标mac地址，配置真实物理服务器集群所有机器虚拟ip和负载均衡服务器ip地址一致，达到不修改数据包的源地址和目标地址，进行数据分发的目的。 实际处理服务器ip和数据请求目的ip一致，不需要经过负载均衡服务器进行地址转换，可将响应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。也称为直接路由模式（DR模式）。

#### 混合

其实这就显而易见了，当单一的负载均衡方式无法很好的解决现有的问题，那么我们就可以把他们结合在一起使用，这也很符合当下的发展潮流啊… 具体的结合方式有很多，例如 我们可以考虑分层，在每一层采用不同的方式来进行负载均衡，在最外层使用 DNS负载均衡，在使用反向代理来做缓存以及动态请求分发 ，最后在是应用负载均衡(IP/DR), 分流到对应的应用集群

![混合一](https://user-gold-cdn.xitu.io/2016/11/29/81bb09e6abe41119cc37b42225d3d529?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

![混合二](https://user-gold-cdn.xitu.io/2016/11/29/b1d311b6d3566a10a99d9509521610c6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

## 具体实现

### 四层

- LVS (Linux Virtual Server) 基于IP层的负载均衡调度技术，它在操作系统核心层上，将来自IP层的TCP/UDP请求均衡地转移到不同的 服务器，从而将一组服务器构成一个高性能、高可用的虚拟服务器。
  1. 抗负载能力强，因为lvs工作方式的逻辑是非常之简单，而且工作在网络4层仅做请求分发之用，没有流量，所以在效率上基本不需要太过考虑。
  2. 配置性低，这通常是一大劣势，但同时也是一大优势，因为没有太多可配置的选项，所以除了增减服务器，并不需要经常去触碰它，大大减少了人为出错的几率。
  3. 工作稳定，因为其本身抗负载能力很强，所以稳定性高也是顺理成章，另外各种lvs都有完整的双机热备方案，所以一点不用担心均衡器本身会出什么问题，节点出现故障的话，lvs会自动判别，所以系统整体是非常稳定的。
  4. 无流量，上面已经有所提及了。lvs仅仅分发请求，而流量并不从它本身出去，所以可以利用它这点来做一些线路分流之用。没有流量同时也保住了均衡器的IO性能不会受到大流量的影响。
  5. 基本上能支持所有应用，因为lvs工作在4层，所以它可以对几乎所有应用做负载均衡，包括http、数据库、聊天室等等.

日PV小于1000万，的时候不需要考虑 LVS , 大多时候LVS + Keepalived配合使用(阿里云),LVS提 供负载均衡，keepalived提供健康检查，故障转移，提高系统的可用性！采用这样的架构以后 很容易对现有系统进行扩展，只要在后端添加或者减少realserver，只要更改lvs的 配置文件，并能实现无缝配置变更！。

### 七层

- HaProxy
  1. HAProxy是工作在网络7层之上。
  2. 能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作
  3. 支持url检测后端的服务器出问题的检测会有很好的帮助。
  4. 更多的负载均衡策略比如：动态加权轮循(Dynamic Round Robin)，加权源地址哈希(Weighted Source Hash)，加权URL哈希和加权参数哈希(Weighted Parameter Hash)已经实现
  5. 单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度。
  6. HAProxy可以对Mysql进行负载均衡，对后端的DB节点进行检测和负载均衡。
- Nignx
  1. 工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构；
  2. Nginx对网络的依赖比较小；
  3. Nginx安装和配置比较简单，测试起来比较方便；
  4. 也可以承担高的负载压力且稳定，一般能支撑超过1万次的并发；
  5. Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测；
  6. Nginx对请求的异步处理可以帮助节点服务器减轻负载；
  7. Nginx能支持http和Email，这样就在适用范围上面小很多；
  8. 不支持Session的保持、对Big request header的支持不是很好，另外默认的只有Round-robin和IP-hash两种负载均衡算法。
- F5 … 牛逼的物理负载均衡设备 土豪配备，性能那是必须的

### 选择

第一阶段：利用Nginx或者HAProxy进行单点的负载均衡，这一阶段服务器规模刚脱离开单服务器、单数据库的模式，需要一定的负载均衡，但是 仍然规模较小没有专业的维护团队来进行维护，也没有需要进行大规模的网站部署。这样利用Nginx或者HAproxy就是第一选择，此时这些东西上手快， 配置容易，在七层之上利用HTTP协议就可以。这时是第一选择

第二阶段：随着网络服务进一步扩大，这时单点的Nginx已经不能满足，这时使用LVS或者商用F5就是首要选择，Nginx此时就作为LVS或者 F5的节点来使用，具体LVS或者F5的是选择是根据公司规模，人才以及资金能力来选择的，这里也不做详谈，但是一般来说这阶段相关人才跟不上业务的提升，所以购买商业负载均衡已经成为了必经之路。

第三阶段：这时网络服务已经成为主流产品，此时随着公司知名度也进一步扩展，相关人才的能力以及数量也随之提升，这时无论从开发适合自身产品的定制，以及降低成本来讲开源的LVS，已经成为首选，这时LVS会成为主流。 最终形成比较理想的状态为：F5/LVS<—>Haproxy<—>Squid/Varnish<—>AppServer。

> LVS/HaProxy/Nignx 的相关介绍摘自网上, 感谢作者，其实网上的介绍都是这样, 我也不知道谁是第一作者了，所以此处就不标注出处了, 望作者原谅

## 负载均衡算法

请求随机分配到各个服务器。

将所有请求，依次分发到每台服务器上，适合服务器硬件同相同的场景。

优先将请求发给拥有最少连接数的后端服务器，常用于长连接服务，例如数据库连接等服务。

将请求的源地址进行hash运算，并结合后端的服务器的权重派发请求至某匹配的服务器，这可以使得同一个客户端IP的请求始终被派发至某特定的服务器。该方式适合负载均衡无cookie功能的TCP协议。

在轮询，随机，最少链接，Hash’等算法的基础上，通过加权的方式，进行负载服务器分配。

# 一、http重定向

当http代理（比如浏览器）向web服务器请求某个URL后，web服务器可以通过http响应头信息中的Location标记来返回一个新的URL。这意味着HTTP代理需要继续请求这个新的URL，完成自动跳转。

性能缺陷：

1、吞吐率限制

主站点服务器的吞吐率平均分配到了被转移的服务器。现假设使用RR（Round Robin）调度策略，子服务器的最大吞吐率为1000reqs/s，那么主服务器的吞吐率要达到3000reqs/s才能完全发挥三台子服务器的作用，那么如果有100台子服务器，那么主服务器的吞吐率可想而知得有大？相反，如果主服务的最大吞吐率为6000reqs/s，那么平均分配到子服务器的吞吐率为2000reqs/s，而现子服务器的最大吞吐率为1000reqs/s，因此就得增加子服务器的数量，增加到6个才能满足。

2、重定向访问深度不同

有的重定向一个静态页面，有的重定向相比复杂的动态页面，那么实际服务器的负载差异是不可预料的，而主站服务器却一无所知。因此整站使用重定向方法做负载均衡不太好。

我们需要权衡转移请求的开销和处理实际请求的开销，前者相对于后者越小，那么重定向的意义就越大，例如下载。你可以去很多镜像下载网站试下，会发现基本下载都使用了Location做了重定向。

# 二、DNS负载均衡

DNS负责提供域名解析服务，当访问某个站点时，实际上首先需要通过该站点域名的DNS服务器来获取域名指向的IP地址，在这一过程中，DNS服务器完成了域名到IP地址的映射，同样，这样映射也可以是一对多的，这时候，DNS服务器便充当了负载均衡调度器，它就像http重定向转换策略一样，将用户的请求分散到多台服务器上，但是它的实现机制完全不同。

使用dig命令来看下"baidu"的DNS设置

![img](https://user-gold-cdn.xitu.io/2017/4/12/ec44afa92b93cc6e60d1dc174437789d.jpg?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

可见baidu拥有三个A记录

相比http重定向，基于DNS的负载均衡完全节省了所谓的主站点，或者说DNS服务器已经充当了主站点的职能。但不同的是，作为调度器，DNS服务器本身的性能几乎不用担心。因为DNS记录可以被用户浏览器或者互联网接入服务商的各级DNS服务器缓存，只有当缓存过期后才会重新向域名的DNS服务器请求解析。也说是DNS不存在http的吞吐率限制，理论上可以无限增加实际服务器的数量。

特性:

1、可以根据用户IP来进行智能解析。DNS服务器可以在所有可用的A记录中寻找离用记最近的一台服务器。

2、动态DNS：在每次IP地址变更时，及时更新DNS服务器。当然，因为缓存，一定的延迟不可避免。

不足：

1、没有用户能直接看到DNS解析到了哪一台实际服务器，加服务器运维人员的调试带来了不便。

2、策略的局限性。例如你无法将HTTP请求的上下文引入到调度策略中，而在前面介绍的基于HTTP重定向的负载均衡系统中，调度器工作在HTTP层面，它可以充分理解HTTP请求后根据站点的应用逻辑来设计调度策略，比如根据请求不同的URL来进行合理的过滤和转移。

3、如果要根据实际服务器的实时负载差异来调整调度策略，这需要DNS服务器在每次解析操作时分析各服务器的健康状态，对于DNS服务器来说，这种自定义开发存在较高的门槛，更何况大多数站点只是使用第三方DNS服务。

4、DNS记录缓存，各级节点的DNS服务器不同程序的缓存会让你晕头转向。

5、基于以上几点，DNS服务器并不能很好地完成工作量均衡分配，最后，是否选择基于DNS的负载均衡方式完全取决于你的需要。

# 三、反向代理负载均衡

这个肯定大家都有所接触，因为几乎所有主流的Web服务器都热衷于支持基于反向代理的负载均衡。它的核心工作就是转发HTTP请求。

相比前面的HTTP重定向和DNS解析，反向代理的调度器扮演的是用户和实际服务器中间人的角色：

1、任何对于实际服务器的HTTP请求都必须经过调度器

2、调度器必须等待实际服务器的HTTP响应，并将它反馈给用户（前两种方式不需要经过调度反馈，是实际服务器直接发送给用户）

特性：

1、调度策略丰富。例如可以为不同的实际服务器设置不同的权重，以达到能者多劳的效果。

2、对反向代理服务器的并发处理能力要求高，因为它工作在HTTP层面。

3、反向代理服务器进行转发操作本身是需要一定开销的，比如创建线程、与后端服务器建立TCP连接、接收后端服务器返回的处理结果、分析HTTP头部信息、用户空间和内核空间的频繁切换等，虽然这部分时间并不长，但是当后端服务器处理请求的时间非常短时，转发的开销就显得尤为突出。例如请求静态文件，更适合使用前面介绍的基于DNS的负载均衡方式。

4、反向代理服务器可以监控后端服务器，比如系统负载、响应时间、是否可用、TCP连接数、流量等，从而根据这些数据调整负载均衡的策略。

5、反射代理服务器可以让用户在一次会话周期内的所有请求始终转发到一台特定的后端服务器上（粘滞会话），这样的好处一是保持session的本地访问，二是防止后端服务器的动态内存缓存的资源浪费。

# 四、IP负载均衡(LVS-NAT)

因为反向代理服务器工作在HTTP层，其本身的开销就已经严重制约了可扩展性，从而也限制了它的性能极限。那能否在HTTP层面以下实现负载均衡呢？

NAT服务器:它工作在传输层，它可以修改发送来的IP数据包，将数据包的目标地址修改为实际服务器地址。

从Linux2.4内核开始，其内置的Neftilter模块在内核中维护着一些数据包过滤表，这些表包含了用于控制数据包过滤的规则。可喜的是，[Linux](https://link.juejin.im/?target=http%3A%2F%2Flib.csdn.net%2Fbase%2Flinux)提供了iptables来对过滤表进行插入、修改和删除等操作。更加令人振奋的是，Linux2.6.x内核中内置了IPVS模块，它的工作性质类型于Netfilter模块，不过它更专注于实现IP负载均衡。

想知道你的服务器内核是否已经安装了IPVS模块，可以

![img](https://user-gold-cdn.xitu.io/2017/4/12/5b6c7795662d9eb3065bb1d35b64cd73.jpg?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

有输出意味着IPVS已经安装了。IPVS的管理工具是ipvsadm，它为提供了基于命令行的配置界面，可以通过它快速实现负载均衡系统。这就是大名鼎鼎的LVS(Linux Virtual Server，Linux虚拟服务器)。

1、打开调度器的数据包转发选项

echo 1 > /proc/sys/net/ipv4/ip_forward

2、检查实际服务器是否已经将NAT服务器作为自己的默认网关，如果不是，如添加

route add default gw xx.xx.xx.xx

3、使用ipvsadm配置

ipvsadm -A -t 111.11.11.11:80 -s rr  

添加一台虚拟服务器，-t 后面是服务器的外网ip和端口，-s rr是指采用简单轮询的RR调度策略（这属于静态调度策略，除此之外，LVS还提供了系列的动态调度策略，比如最小连接（LC）、带权重的最小连接（WLC），最短期望时间延迟（SED）等）

ipvsadm -a -t 111.11.11.11:80 -r 10.10.120.210:8000 -m  

ipvsadm -a -t 111.11.11.11:80 -r 10.10.120.211:8000 -m  

添加两台实际服务器（不需要有外网ip），-r后面是实际服务器的内网ip和端口，-m表示采用NAT方式来转发数据包

运行ipvsadm -L -n可以查看实际服务器的状态。这样就大功告成了。

实验证明使用基于NAT的负载均衡系统。作为调度器的NAT服务器可以将吞吐率提升到一个新的高度，几乎是反向代理服务器的两倍以上，这大多归功于在内核中进行请求转发的较低开销。但是一旦请求的内容过大时，不论是基于反向代理还是NAT，负载均衡的整体吞吐量都差距不大，这说明对于一睦开销较大的内容，使用简单的反向代理来搭建负载均衡系统是值考虑的。

这么强大的系统还是有它的瓶颈，那就是NAT服务器的网络带宽，包括内部网络和外部网络。当然如果你不差钱，可以去花钱去购买千兆交换机或万兆交换机，甚至负载均衡硬件设备，但如果你是个屌丝，咋办？

一个简单有效的办法就是将基于NAT的集群和前面的DNS混合使用，比如５个100Mbps出口宽带的集群，然后通过DNS来将用户请求均衡地指向这些集群，同时，你还可以利用DNS智能解析实现地域就近访问。这样的配置对于大多数业务是足够了，但是对于提供下载或视频等服务的大规模站点，NAT服务器还是不够出色。

# 五、直接路由(LVS-DR)

NAT是工作在网络分层模型的传输层（第四层），而直接路由是工作在数据链路层（第二层），貌似更屌些。它通过修改数据包的目标MAC地址（没有修改目标IP），将数据包转发到实际服务器上，不同的是，实际服务器的响应数据包将直接发送给客户羰，而不经过调度器。

1、网络设置

这里假设一台负载均衡调度器，两台实际服务器，购买三个外网ip，一台机一个，三台机的默认网关需要相同，最后再设置同样的ip别名，这里假设别名为10.10.120.193。这样一来，将通过10.10.120.193这个IP别名来访问调度器，你可以将站点的域名指向这个IP别名。

2、将ip别名添加到回环接口lo上

这是为了让实际服务器不要去寻找其他拥有这个IP别名的服务器，在实际服务器中运行：

![img](https://user-gold-cdn.xitu.io/2017/4/12/a06c6f28d2d9f7f46decdb82e120a9c6.jpg?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

另外还要防止实际服务器响应来自网络中针对IP别名的ARP广播，为此还要执行：

echo "1" > /proc/sys/net/ipv4/conf/lo/arp_ignore

echo "2" > /proc/sys/net/ipv4/conf/lo/arp_announce

echo "1" > /proc/sys/net/ipv4/conf/all/arp_ignore

echo "1" > /proc/sys/net/ipv4/conf/all/arp_announce

配置完了就可以使用ipvsadm配置LVS-DR集群了

ipvsadm -A -t 10.10.120.193:80 -s rr  

ipvsadm -a -t 10.10.120.193:80 -r 10.10.120.210:8000 -g  

ipvsadm -a -t 10.10.120.193:80 -r 10.10.120.211:8000 -g  

-g 就意味着使用直接路由的方式转发数据包

LVS-DR 相较于LVS-NAT的最大优势在于LVS-DR不受调度器宽带的限制，例如假设三台服务器在WAN交换机出口宽带都限制为10Mbps，只要对于连接调度器和两台实际服务器的LAN交换机没有限速，那么，使用LVS-DR理论上可以达到20Mbps的最大出口宽带，因为它的实际服务器的响应数据包可以不经过调度器而直接发往用户端啊，所以它与调度器的出口宽带没有关系，只能自身的有关系。而如果使用LVS-NAT，集群只能最大使用10Mbps的宽带。所以，越是响应数据包远远超过请求数据包的服务，就越应该降低调度器转移请求的开销，也就越能提高整体的扩展能力，最终也就越依赖于WAN出口宽带。

总的来说，LVS-DR适合搭建可扩展的负载均衡系统，不论是Web服务器还是文件服务器，以及视频服务器，它都拥有出色的性能。前提是你必须为实际器购买一系列的合法IP地址。

# 六、IP隧道(LVS-TUN)

基于IP隧道的请求转发机制：将调度器收到的IP数据包封装在一个新的IP数据包中，转交给实际服务器，然后实际服务器的响应数据包可以直接到达用户端。目前Linux大多支持，可以用LVS来实现，称为LVS-TUN，与LVS-DR不同的是，实际服务器可以和调度器不在同一个WANt网段，调度器通过IP隧道技术来转发请求到实际服务器，所以实际服务器也必须拥有合法的IP地址。

总体来说，LVS-DR和LVS-TUN都适合响应和请求不对称的Web服务器，如何从它们中做出选择，取决于你的网络部署需要，因为LVS-TUN可以将实际服务器根据需要部署在不同的地域，并且根据就近访问的原则来转移请求，所以有类似这种需求的，就应该选择LVS-TUN。





负载均衡是什么鬼？从字面意思来看，它应该有两层意思分别是负载和均衡。而对于系统负载均衡它同样具有两层意思，其中系统负载指的系统能够承载的最大访问流量，系统均衡指的是前端请求要**均匀地分配**给后端机器，同时，同一用户要尽可能分配给同一机器。系统通过负载均衡以后具有如下好处：

1、**避免资源浪费**。如果我们均衡算法选的不好，就会导致后端资源浪费。比如：如果选择一致Hash算法，可以很好利用cache的容量。而如果用随机，有可能会让cache效果大打折扣。

2、**避免服务不可用**。当我们不考虑系统的承载能力，有可能直接把某台机器压垮，比如当机器的CPU利用率达到80%，如果再有大量的请求，那么该机器直接宕机，甚至于导致雪崩情况（一台机器宕机，对应的请求会分给其他机器上，那么其他机器也会出现宕机，以至于全部机器都宕机）。

## 理论基础

系统要实现负载均衡，背后肯定需要一些算法支撑，下面就来看下对应的算法。

**1、负载算法**

既然要解决后端系统的承载能力，那我们就有很多方式，常用的有以下几种：

**静态配置**

这种方式对于中小系统来讲是最有效最稳定的。因为后端机器的性能配置、上面部署哪些服务，还能有多大的承载能力等等，我们是最清楚的。比如，我们经常看到nginx的配置：

![image](https://user-gold-cdn.xitu.io/2018/6/5/163ce433c611f0d0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

**动态调整**

当碰到机器故障，以及由于性能问题无法处理请求时，如果此时还将新来的请求分配到该节点，那么可能导致该节点宕机。因此，根据节点的实际负载动态调整节点的权重是非常重要的。当然，要得到节点的真正负载，以及如何定义负载，无论负载是否及时收集，都是需要考虑的问题。

动态调整首先计算所有节点的请求响应时间，对于响应较快的节点，我们可以多分配请求给它，然后增加它的请求数，当它的响应变慢时再慢慢减少它的请求数，慢慢地我们找到这个节点最佳平衡点，即分配多少请求给它。通过相同方法我们找到所有节点的平衡点。

这种方法的好处在于可以动态平衡后面服务器的处理能力。不过，任何事物都有两面性。这种方案如果遇到极端情况，可能会造成雪崩。当某台机器出现短暂网络抖动的时候，它的响应就可能变慢，这个时候，前端服务就会将它的请求分配给其他机器。如果分配的很多，就有可能造成某些机器响应也变慢。然后又将这些机器的请求分配给另外机器。如此这般，就会造成雪崩。

**2、均衡算法**

均衡算法主要解决将请求如何发送给后端服务。经常会用到以下三种算法：随机（random）、轮询（round-robin）和 哈希算法。

**随机算法**

随机算法就是通过一个随机函数将所有请求分散到各个节点上， 该方法比较简单，且能做到将请求均匀地分散到各个节点上，因此经常使用随机算法。

**轮询算法**

轮询算法就是将所有节点以同样的概率向外提供服务，但是它没有考虑各个节点之间的性能差别，对于同样数目的请求，性能好的节点能够轻松完成，而性能差的节点完成的比较费力。因此，我们提出了加权轮询算法，为不同性能的节点赋予不同权重。

**哈希算法**

通常将用户 id 或 ip 作为key，计算出对应的hash值，然后再对节点数量取模，即hash(key) mode n，其中n为节点数，得到该用户请求落到哪个节点上。该方法可以做到让同一个请求落到同一个节点中，但是当节点数量发生动态变化时，该方法就不太适应了。此时，就应该使用一致性hash算法。一致性hash算法就是把每台server分成v个虚拟节点，再把所有虚拟节点（n*v）随机分配到一致性哈希的圆环上，这样所有的用户从自己圆环上的位置顺时针往下取到第一个vnode就是自己所属的节点。当此节点存在故障时，再顺时针取下一个作为替代节点。更加具体的描述可以参考[一致性Hash](https://link.juejin.im?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU1NTQwNzIzNw%3D%3D%26mid%3D2247483758%26idx%3D1%26sn%3D3a8e8110c64985861b31130c6b15fa39%26chksm%3Dfbd58384cca20a92e5cf2c8be7bfb4e2125c258bdac7e36c0540a76672d55a9d2373c5994abb%26scene%3D21%23wechat_redirect)这篇文章，这里就不展开了。

## 具体实现

目前负载均衡系统有Nginx、LVS、F5，其总会难过Nginx是软件的7层负载均衡，LVS是内核的4层负载均衡，F5是硬件的4层负载均衡。

软件和硬件的区别在于性能，硬件远远高于软件，Nginx的性能是万级的，一般的Linux服务器上安装一个Nginx能达到每秒5万并发请求；而F5的性能能达到百万级，从200万每秒到800万每秒都有，不过价格很贵。

4层和7层的区别在于协议和灵活性，Nginx是7层的，它支持HTTP等协议，而LVS和F5是4层协议，它们和协议无关，几乎所有应用都可以做