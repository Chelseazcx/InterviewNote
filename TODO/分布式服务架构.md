# 分布式服务架构

## 微服务

服务代理模式：

![23210452_bXd4.png](http://static.oschina.net/uploads/img/201508/23210452_bXd4.png)

服务聚合模式：客户端并不聚合数据，但会根据业务需求的差别调用不同的微服务。

![23210431_Tj9N.png](http://static.oschina.net/uploads/img/201508/23210431_Tj9N.png)

服务串联模式：服务A接收到请求后会与服务B进行通信，类似地，服务B会同服务C进行通信。所有服务都使用同步消息传递。

![23210518_ZiiS.png](http://static.oschina.net/uploads/img/201508/23210518_ZiiS.png)

服务分支模式：允许同时调用两个微服务链

![23210547_OjkK.png](http://static.oschina.net/uploads/img/201508/23210547_OjkK.png)

服务异步消息模式：选择使用消息队列代替REST请求/响应

![23210720_tCCb.png](http://static.oschina.net/uploads/img/201508/23210720_tCCb.png)

服务共享数据模式：部分微服务可能会共享缓存和数据库存储。不过，这只有在两个服务之间存在强耦合关系时才可以。

![23210717_JDyF.png](http://static.oschina.net/uploads/img/201508/23210717_JDyF.png)



平滑的系统迁移

1. 在新老系统上双写
2. 迁移双写之前的历史遗留数据
3. 将读请求切换到新系统
4. 下调双写逻辑，只写新系统




## 微服务容错模型

舱壁隔离模型

- 微服务容器分组

  准生产环境：提供内侧使用

  灰度环境：普通用户

  生产环境：大部分用户和VIP用户

在一次比较大的重构过程中，可以充分利用灰度环境的隔离性进行预验证，普通用户验证没有问题后，再上生产环境

![img](https://images2018.cnblogs.com/blog/486074/201807/486074-20180707123147893-1143996945.png)

- 线程池隔离

  将同一类功能划分在一个微服务中，尽量避免微服务过细而导致成本增加，适可而止。

![img](https://images2018.cnblogs.com/blog/486074/201807/486074-20180707123202802-402419776.png)

- 熔断模式

当服务的输入负载迅速增加时，如果没有有效的措施对负载进行熔断，则会使服务迅速被压垮，服务被压垮会导致依赖的服务都被压垮，出现雪崩效应，因此，可通过模拟家庭的电路保险开关，在微服务架构中实现熔断模式。

![img](https://images2018.cnblogs.com/blog/486074/201807/486074-20180707123219316-557516910.png)

- 限流模式

针对服务突然上量，我们必须有限流机制，限流机制一般会控制访问的并发量，例如每秒允许处理的并发用户数及查询量、请求量等。

​	计数器：通过原子变量计算单位时间内的访问次数，如果超出某个阈值，则拒绝后续的请求，等到下一个单位时间再重新计数。

​	在计数器的实现方法中通常定义了一个循环数组（见图1-30），例如：定义5个元素的环形数组，计数周期为1s，可以记录4s内的访问量，其中有1个元素为当前时间点的标志，通常来说每秒程序都会将前面3s的访问量打印到日志，供统计分析。

![img](http://image109.360doc.com/DownloadImg/2018/02/2208/125221266_3_20180222085713691)

我们将时间的秒数除以数组元素的个数5，然后取模，映射到环形数组里的数据元素，假如当前时间是1 000 000 002s，那么对应当前时间的环形数组里的第3个元素，下标为2。此时的数组元素的数据如图所示。![img](http://image109.360doc.com/DownloadImg/2018/02/2208/125221266_4_20180222085714597)

这时程序可以对第3个元素即下标为2的元素，进行累加并判断是否达到阈值，如果达到阈值，则拒绝请求，否则请求通过；

​	令牌筒：它通过一个线程在单位时间内生产固定数量的令牌，然后把令牌放入队列，每次请求调用需要从桶中拿取一个令牌，拿到令牌后才有资格执行请求调用，否则只能等待拿到令牌再执行，或者直接丢弃。

![img](https://images2018.cnblogs.com/blog/486074/201807/486074-20180707123243024-750018005.png)

​	信号量：限流类似于生活中的漏洞，无论倒入多少油，下面有漏管的流量是有限的，实际上我们在应用层使用的信号量也可以实现限流。

- 失效转移模式

若微服务架构中发生了熔断和限流，则该如何处理被拒绝的请求呢？解决这个问题的模式叫作失效转移模式，通常分为下面几种。

- - 采用快速失败的策略，直接返回使用方错误，让使用方知道发生了问题并自行决定后续处理。
  - 是否有备份服务，如果有备份服务，则迅速切换到备份服务。
  - 失败的服务有可能是某台机器有问题，而不是所有机器有问题，例如OOM问题，在这种情况下适合使用failover策略，采用重试的方法来解决，但是这种方法要求服务提供者的服务实现了幂等性。

## 分布式一致性

## 一致性问题

- 下订单和扣库存：先下单，在扣库存，就会导致超卖。下单不成功，扣库存成功，导致少买。

- 同步调用超时：系统A同步调用系统B超时，系统A可以明确得到超时反馈，但是无法确定系统B是否已经完成了预设功能，这时系统A不知道该如何反馈给使用方。

- 异步回调超时：系统A发出请求调用系统B，系统B受理后则返回成功信息，然后系统B处理后异步通知系统A结果。在这个过程中，如果系统A由于某种原因没有收到回调结果，这两个系统的状态就不一致了。

- 掉单：两个系统协处理一个流程，分别为对方的上下游，如果一个系统中存在一个请求（订单），另外一个系统不存在，则会导致掉单。

- 系统间状态不一致：两个系统都存在请求，但是请求的状态不一致

- 缓存和数据库不一致：缓存与数据库之间如何保持一致性

- 本地缓存节点间不一致：一个服务池上多个节点之间的状态不一致。

- 缓存数据结构不一致：由于程序处理不当，将不完整的数据存入缓存中，缓存使用者很可能由于数据的不完整抛异常。

  ### 一致性模式和思路

  **酸碱平衡理论**

  - **ACID（酸）**

      A: Atomicity，原子性

      C: Consistency，一致性

      I: Isolation，隔离性

      D: Durability，持久性

  关系型数据库天生用于解决具有复杂事务场景的问题，完全满足ACID的特性。

  具有ACID特性的数据库支持强一致性，强一致性代表数据库本身不会出现不一致，每个事务都是原子的，或者成功或者失败，事务间是隔离的，互相完全不受影响，而且最终状态是持久落盘的。

  > NoSQL完全不适合交易场景，主要用来做数据分析、ETL、报表、数据挖掘、推荐、日志处理、调用链跟踪等非核心交易场景。

  现在我们来看看下订单和扣库存一致性问题，如果是在数据量较小的情况下，可以利用关系型数据库的强一致性解决，也就是把订单表和库存表放在同一个关系型数据库中，利用关系型数据库进行下订单和扣库存两个紧密相关的操作，达到订单和库存实时一致的结果。如果是大规模高并发的情况，由于业务规则的限制，无法将相关数据分到同一个数据库分片，这时就需要实现**最终一致性**。

  - **CAP（帽子原理）**

  由于对系统或者数据进行了拆分，我们的系统不再是单机系统，而是分布式系统。

      C: Consistency，一致性。在分布式系统中的所有数据备份，在同一时刻具有同样的值，所有节点在同一时刻读取的数据都是最新的数据副本。

      A: Availability，可用性，好的响应性能。完全的可用性指的是在任何故障模型下，服务都会在有限的时间内处理完成并进行响应。

      P:  Partition tolerance，分区容忍性。尽管网络上有部分消息丢失，但系统仍然可继续工作。

  CAP原理证明，任何分布式系统只可同时满足以上两点，无法三者兼顾。由于关系型数据库是单节点无复制的，因此不具有分区容忍性，但是具有一致性和可用性，而分布式的服务化系统都需要满足分区容忍性，那么我们必须在一致性和可用性之间进行权衡。

  - **BASE（碱）**

  BASE思想解决了CAP提出的分布式系统的一致性和可用性不可兼得的问题。BASE思想与ACID原理截然不同，它满足CAP原理，通过牺牲强一致性获得可用性，一般应用于服务化系统的应用层或者大数据处理系统中，通过达到最终一致性来尽量满足业务的绝大多数需求。

  酸碱平衡理论，简单来说就是在不同的场景下，可以分别利用ACID和BASE来解决分布式服务化系统的一致性问题。

      BA: Basically Available，基本可用

      S: Soft State，软状态，状态可以在一段时间内不同步

      E: Eventually Consistent，最终一致，在一定的时间窗口内，最终数据达成一致即可

  软状态是实现BASE思想的方法，基本可用和最终一致是目标。以BASE思想实现的系统由于不保证强一致性，所有系统在处理请求的过程中存在短暂的不一致，在短暂的不一致的时间窗口内，请求处理处于临时状态中，系统在进行每步操作时，通过记录每个临时状态，在系统出现故障时可以从这些中间状态继续处理未完成的请求或者退回到原始状态，最终达到一致状态。

  - **对酸碱平衡的总结**

  解决一致性问题的三条实践经验：使用向上扩展（强悍的硬件）并运行专业的关系型数据库，能够保证强一致性；关系型数据库水平伸缩和分片，将相关数据分到数据库的同一个片上；最终一致性。

## 分布式一致性协议

### 两阶段提交协议

两阶段提交协议把分布式事务分成两个过程，一个是准备阶段，一个是提交阶段，准备阶段和提交阶段都是由事务管理器发起的，为了接下来讲解方便，我们把事务管理器称为协调者，把资管管理器称为参与者。

两阶段如下：

1. 准备阶段：协调者向参与者发起指令，参与者评估自己的状态，如果参与者评估指令可以完成，参与者会写redo或者undo日志（这也是前面提起的Write-Ahead Log的一种），然后锁定资源，执行操作，但是并不提交
2. 提交阶段：如果每个参与者明确返回准备成功，也就是预留资源和执行操作成功，协调者向参与者发起提交指令，参与者提交资源变更的事务，释放锁定的资源；如果任何一个参与者明确返回准备失败，也就是预留资源或者执行操作失败，协调者向参与者发起中止指令，参与者取消已经变更的事务，执行undo日志，释放锁定的资源

两阶段提交协议成功场景示意图如下：

![f1a038cc187255e69d30836a0ad5bc9cb5f65cbd](https://yqfile.alicdn.com/f1a038cc187255e69d30836a0ad5bc9cb5f65cbd.png)

我们看到两阶段提交协议在准备阶段锁定资源，是一个重量级的操作，并能保证强一致性，但是实现起来复杂、成本较高，不够灵活，更重要的是它有如下致命的问题：

1. 阻塞：从上面的描述来看，对于任何一次指令必须收到明确的响应，才会继续做下一步，否则处于阻塞状态，占用的资源被一直锁定，不会被释放
2. 单点故障：如果协调者宕机，参与者没有了协调者指挥，会一直阻塞，尽管可以通过选举新的协调者替代原有协调者，但是如果之前协调者在发送一个提交指令后宕机，而提交指令仅仅被一个参与者接受，并且参与者接收后也宕机，新上任的协调者无法处理这种情况
3. 脑裂：协调者发送提交指令，有的参与者接收到执行了事务，有的参与者没有接收到事务，就没有执行事务，多个参与者之间是不一致的

上面所有的这些问题，都是需要人工干预处理，没有自动化的解决方案，因此两阶段提交协议在正常情况下能保证系统的强一致性，但是在出现异常情况下，当前处理的操作处于错误状态，需要管理员人工干预解决，因此可用性不够好，这也符合CAP协议的一致性和可用性不能兼得的原理。

### 三阶段提交协议

三阶段提交协议是两阶段提交协议的改进版本。它通过超时机制解决了阻塞的问题，并且把两个阶段增加为三个阶段：

1. 询问阶段：协调者询问参与者是否可以完成指令，协调者只需要回答是还是不是，而不需要做真正的操作，这个阶段超时导致中止
2. 准备阶段：如果在询问阶段所有的参与者都返回可以执行操作，协调者向参与者发送预执行请求，然后参与者写redo和undo日志，执行操作，但是不提交操作；如果在询问阶段任何参与者返回不能执行操作的结果，则协调者向参与者发送中止请求，这里的逻辑与两阶段提交协议的的准备阶段是相似的，这个阶段超时导致成功
3. 提交阶段：如果每个参与者在准备阶段返回准备成功，也就是预留资源和执行操作成功，协调者向参与者发起提交指令，参与者提交资源变更的事务，释放锁定的资源；如果任何一个参与者返回准备失败，也就是预留资源或者执行操作失败，协调者向参与者发起中止指令，参与者取消已经变更的事务，执行undo日志，释放锁定的资源，这里的逻辑与两阶段提交协议的提交阶段一致

三阶段提交协议成功场景示意图如下：

1. 询问阶段：协调者询问参与者是否可以完成指令，协调者只需要回答是还是不是，而不需要做真正的操作，这个阶段超时导致中止
2. 准备阶段：如果在询问阶段所有的参与者都返回可以执行操作，协调者向参与者发送预执行请求，然后参与者写redo和undo日志，执行操作，但是不提交操作；如果在询问阶段任何参与者返回不能执行操作的结果，则协调者向参与者发送中止请求，这里的逻辑与两阶段提交协议的的准备阶段是相似的，这个阶段超时导致成功
3. 提交阶段：如果每个参与者在准备阶段返回准备成功，也就是预留资源和执行操作成功，协调者向参与者发起提交指令，参与者提交资源变更的事务，释放锁定的资源；如果任何一个参与者返回准备失败，也就是预留资源或者执行操作失败，协调者向参与者发起中止指令，参与者取消已经变更的事务，执行undo日志，释放锁定的资源，这里的逻辑与两阶段提交协议的提交阶段一致
4. 然而，这里与两阶段提交协议有两个主要的不同：
5. 增加了一个询问阶段，询问阶段可以确保尽可能早的发现无法执行操作而需要中止的行为，但是它并不能发现所有的这种行为，只会减少这种情况的发生
6. 在准备阶段以后，协调者和参与者执行的任务中都增加了超时，一旦超时，协调者和参与者都继续提交事务，默认为成功，这也是根据概率统计上超时后默认成功的正确性最大

三阶段提交协议与两阶段提交协议相比，具有如上的优点，但是一旦发生超时，系统仍然会发生不一致，只不过这种情况很少见罢了，好处就是至少不会阻塞和永远锁定资源。

### *3. TCC*

上面两节讲解了两阶段提交协议和三阶段提交协议，实际上他们能解决案例2-转账和案例3-下订单和扣库存中的分布式事务的问题，但是遇到极端情况，系统会发生阻塞或者不一致的问题，需要运营或者技术人工解决。无论两阶段还是三阶段方案中都包含多个参与者、多个阶段实现一个事务，实现复杂，性能也是一个很大的问题，因此，在互联网高并发系统中，鲜有使用两阶段提交和三阶段提交协议的场景。

阿里巴巴提出了新的TCC协议，TCC协议将一个任务拆分成Try、Confirm、Cancel，正常的流程会先执行Try，如果执行没有问题，再执行Confirm，如果执行过程中出了问题，则执行操作的逆操Cancel，从正常的流程上讲，这仍然是一个两阶段的提交协议，但是，在执行出现问题的时候，有一定的自我修复能力，如果任何一个参与者出现了问题，协调者通过执行操作的逆操作来取消之前的操作，达到最终的一致状态。

可以看出，从时序上，如果遇到极端情况下TCC会有很多问题的，例如，如果在Cancel的时候一些参与者收到指令，而一些参与者没有收到指令，整个系统仍然是不一致的，这种复杂的情况，系统首先会通过补偿的方式，尝试自动修复的，如果系统无法修复，必须由人工参与解决。

从TCC的逻辑上看，可以说TCC是简化版的三阶段提交协议，解决了两阶段提交协议的阻塞问题，但是没有解决极端情况下会出现不一致和脑裂的问题。然而，TCC通过自动化补偿手段，会把需要人工处理的不一致情况降到到最少，也是一种非常有用的解决方案，根据线人，阿里在内部的一些中间件上实现了TCC模式。

我们给出一个使用TCC的实际案例，在秒杀的场景，用户发起下单请求，应用层先查询库存，确认商品库存还有余量，则锁定库存，此时订单状态为待支付，然后指引用户去支付，由于某种原因用户支付失败，或者支付超时，系统会自动将锁定的库存解锁供其他用户秒杀。

TCC协议使用场景示意图如下：![cc8621dc68b433b1a60639ae26c14e4afd4edc56](https://yqfile.alicdn.com/cc8621dc68b433b1a60639ae26c14e4afd4edc56.png)



总结一下，两阶段提交协议、三阶段提交协议、TCC协议都能保证分布式事务的一致性，他们保证的分布式系统的一致性从强到弱，TCC达到的目标是最终一致性，其中任何一种方法都可以不同程度的解决案例2：转账、案例3：下订单和扣库存的问题，只是实现的一致性的级别不一样而已，对于案例4：同步超时可以通过TCC的理念解决，如果同步调用超时，调用方可以使用fastfail策略，返回调用方的使用方失败的结果，同时调用服务的逆向cancel操作，保证服务的最终一致性。



## 保证最终一致性的模式

在大规模高并发服务化系统中，一个功能被拆分成多个具有单一功能的元功能，一个流程会有多个系统的多个元功能组合实现，如果使用两阶段提交协议和三阶段提交协议，确实能解决系统间一致性问题，除了这两个协议带来的自身的问题，这些协议的实现比较复杂、成本比较高，最重要的是性能并不好，相比来看，TCC协议更简单、容易实现，但是TCC协议由于每个事务都需要执行Try，再执行Confirm，略微显得臃肿，因此，在现实的系统中，底线要求仅仅需要能达到最终一致性，而不需要实现专业的、复杂的一致性协议，实现最终一致性有一些非常有效的、简单粗暴的模式，下面就介绍这些模式及其应用场景。

#### 1. 查询模式

任何一个服务操作都需要提供一个查询接口，用来向外部输出操作执行的状态。服务操作的使用方可以通过查询接口，得知服务操作执行的状态，然后根据不同状态来做不同的处理操作。

为了能够实现查询，每个服务操作都需要有唯一的流水号标识，也可使用此次服务操作对应的资源ID来标志，例如：请求流水号、订单号等。

首先，单笔查询操作是必须提供的，我们也鼓励使用单笔订单查询，这是因为每次调用需要占用的负载是可控的，批量查询则根据需要来提供，如果使用了批量查询，需要有合理的分页机制，并且必须限制分页的大小，以及对批量查询的QPS需要有容量评估和流控等。

查询模式的示意图如下：

![img](https://upload-images.jianshu.io/upload_images/4310879-3f9201ad2f0e60e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

查询模式

对于*案例4：同步超时、案例5：异步回调超时、案例6：掉单、案例7：系统间状态不一致*，我们都需要使用查询模式来了解被调用服务的处理情况，来决定下一步做什么：补偿未完成的操作还是回滚已经完成的操作。

#### *2. 补偿模式*

有了上面的查询模式，在任何情况下，我们都能得知具体的操作所处的状态，如果整个操作处于不正常的状态，我们需要修正操作中有问题的子操作，这可能需要重新执行未完成的子操作，后者取消已经完成的子操作，通过修复使整个分布式系统达到一致，为了让系统最终一致而做的努力都叫做补偿。

对于服务化系统中同步调用的操作，业务操作发起的主动方在还没有得到业务操作执行方的明确返回或者调用超时，场景可参考*案例4：同步超时*，这个时候业务发起的主动方需要及时的调用业务执行方获得操作执行的状态，这里使用查询模式，获得业务操作的执行方的状态后，如果业务执行方已经完预设的工作，则业务发起方给业务的使用方返回成功，如果业务操作的执行方的状态为失败或者未知，则会立即告诉业务的使用方失败，然后调用业务操作的逆向操作，保证操作不被执行或者回滚已经执行的操作，让业务的使用方、业务发起的主动方、业务的操作方最终达成一致的状态。

补偿模式的示意图如下：

![img](//upload-images.jianshu.io/upload_images/4310879-ae502df685232c5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/963/format/webp)

补偿模式

补偿操作根据发起形式分为：

> 1. 自动恢复：程序根据发生不一致的环境，通过继续未完成的操作，或者回滚已经完成的操作，自动来达到一致

1. 通知运营：如果程序无法自动恢复，并且设计时考虑到了不一致的场景，可以提供运营功能，通过运营手工进行补偿
2. 通知技术：如果很不巧，系统无法自动回复，又没有运营功能，那必须通过技术手段来解决，技术手段包括走数据库变更或者代码变更来解决，这是最糟的一种场景

#### *3. 异步确保模式*

异步确保模式是补偿模式的一个典型案例，经常应用到使用方对响应时间要求并不太高，我们通常把这类操作从主流程中摘除，通过异步的方式进行处理，处理后把结果通过通知系统通知给使用方，这个方案最大的好处能够对高并发流量进行消峰，例如：电商系统中的物流、配送，以及支付系统中的计费、入账等。

实践中，将要执行的异步操作封装后持久入库，然后通过定时捞取未完成的任务进行补偿操作来实现异步确保模式，只要定时系统足够健壮，任何一个任务最终会被成功执行。

异步确保模式的示意图如下：

![img](//upload-images.jianshu.io/upload_images/4310879-80f0bd6afee2dec9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

异步确保模式

对于*案例5：异步回调超时*，使用的就是异步确保模式，这种情况下对于某个操作，如果迟迟没有收到响应，我们通过查询模式和补偿模式来继续未完成的操作。

#### *4. 定期校对模式*

既然我们在系统中实现最终一致性，系统在没有达到一致之前，系统间的状态是不一致的，甚至是混乱的，需要补偿操作来达到一致的目的，但是我们如何来发现需要补偿的操作呢？

在操作的主流程中的系统间执行校对操作，我们可以事后异步的批量校对操作的状态，如果发现不一致的操作，则进行补偿，补偿操作与补偿模式中的补偿操作是一致的。

另外，实现定期校对的一个关键就是分布式系统中需要有一个自始至终唯一的ID，ID的生成请参考[SnowFlake](https://link.jianshu.com?t=https://github.com/twitter/snowflake)。

在分布式系统中，全局唯一ID的示意图如下：

![img](//upload-images.jianshu.io/upload_images/4310879-0ca903fe2371c0b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

唯一ID

一般情况下，生成全局唯一ID有两种方法：

> 1. 持久型：使用数据库表自增字段或者Sequence生成，为了提高效率，每个应用节点可以缓存一批次的ID，如果机器重启可能会损失一部分ID，但是这并不会产生任何问题

1. 时间型：一般由机器号、业务号、时间、单节点内自增ID组成，由于时间一般精确到秒或者毫秒，因此不需要持久就能保证在分布式系统中全局唯一、粗略递增能特点

实践中，为了能在分布式系统中迅速的定位问题，一般的分布式系统都有技术支持系统，它能够跟踪一个请求的调用链，调用链是在二维的维度跟踪一个调用请求，最后形成一个调用树，原理可参考谷歌的论文[Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](https://link.jianshu.com?t=https://research.google.com/pubs/pub36356.html)，一个开源的参考实现为[pinpoint](https://link.jianshu.com?t=https://github.com/naver/pinpoint)。

在分布式系统中，调用链的示意图如下：

![img](//upload-images.jianshu.io/upload_images/4310879-83b4c51ec06865ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

调用链

全局的唯一流水ID可以把一个请求在分布式系统中的流转的路径聚合，而调用链中的spanid可以把聚合的请求路径通过树形结构进行展示，让技术支持人员轻松的发现系统出现的问题，能够快速定位出现问题的服务节点，提高应急效率。

关于订单跟踪、调用链跟踪、业务链跟踪，我们会在后续文章中详细介绍。

在分布式系统中构建了唯一ID，调用链等基础设施，我们很容易对系统间的不一致进行核对，通常我们需要构建第三方的定期核对系统，以第三方的角度来监控服务执行的健康程度。

定期核对系统示意图如下：

![img](//upload-images.jianshu.io/upload_images/4310879-11d5f1aa70465546.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/997/format/webp)

定期核对模式

对于*案例6：掉单、案例7：系统间状态不一致*通常通过定期校对模式发现问题，并通过补偿模式来修复，最后完成系统间的最终一致性。

定期校对模式多应用在金融系统，金融系统由于涉及到资金安全，需要保证百分之百的准确性，所以，需要多重的一致性保证机制，包括：系统间的一致性对账、现金对账、账务对账、手续费对账等等，这些都属于定期校对模式，顺便说一下，金融系统与社交应用在技术上本质的区别在于社交应用在于量大，而金融系统在于数据的准确性。

到现在为止，我们看到通过查询模式、补偿模式、定期核对模式可以解决案例4到案例7的所有问题，对于*案例4：同步超时*，如果同步超时，我们需要查询状态进行补偿，对于*案例5：异步回调超时*，如果迟迟没有收到回调响应，我们也会通过查询状态进行补偿，对于*案例6：掉单、案例7：系统间状态不一致*，我们通过定期核对模式可以保证系统间操作的一致性，避免掉单和状态不一致导致问题。

#### *5. 可靠消息模式*

在分布式系统中，对于主流程中优先级比较低的操作，大多采用异步的方式执行，也就是前面提到的异步确保型，为了让异步操作的调用方和被调用方充分的解耦，也由于专业的消息队列本身具有可伸缩、可分片、可持久等功能，我们通常通过消息队列实现异步化，对于消息队列，我们需要建立特殊的设施保证可靠的消息发送以及处理机的幂等等。

**消息的可靠发送**

消息的可靠发送可以认为是尽最大努力发送消息通知，有两种实现方法：

第一种，发送消息之前，把消息持久到数据库，状态标记为待发送，然后发送消息，如果发送成功，将消息改为发送成功。定时任务定时从数据库捞取一定时间内未发送的消息，将消息发送。

![img](//upload-images.jianshu.io/upload_images/4310879-230c62dbf98ccfb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

消息发送模式1

第二种，实现方式与第一种类似，不同的是持久消息的数据库是独立的，并不耦合在业务系统中。发送消息之前，先发送一个预消息给某一个第三方的消息管理器，消息管理器将其持久到数据库，并标记状态为待发送，发送成功后，标记消息为发送成功。定时任务定时从数据库捞取一定时间内未发送的消息，回查业务系统是否要继续发送，根据查询结果来确定消息的状态。

![img](//upload-images.jianshu.io/upload_images/4310879-bf4f4d60e66b37f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

消息发送模式2

一些公司把消息的可靠发送实现在了中间件里，通过Spring的注入，在消息发送的时候自动持久消息记录，如果有消息记录没有发送成功，定时会补偿发送。

**消息处理器的幂等性**

如果我们要保证消息可靠的发送，简单来说，要保证消息一定要发送出去，那么就需要有重试机制，有了重试机制，消息一定会重复，那么我们需要对重复做处理。

处理重复的最佳方式为保证操作的幂等性，幂等性的数学公式为：

> f(f(x)) = f(x)

保证操作的幂等性常用的几个方法：

> 1. 使用数据库表的唯一键进行滤重，拒绝重复的请求

1. 使用分布式表对请求进行滤重
2. 使用状态流转的方向性来滤重，通常使用行级锁来实现(后续在锁相关的文章中详细说明)
3. 根据业务的特点，操作本身就是幂等的，例如：删除一个资源、增加一个资源、获得一个资源等

#### *6. 缓存一致性模型*

大规模高并发系统中一个常见的核心需求就是亿级的读需求，显然，关系型数据库并不是解决高并发读需求的最佳方案，互联网的经典做法就是使用缓存抗读需求，下面有一些使用缓存的保证一致性的最佳实践：

> 1. 如果性能要求不是非常的高，尽量使用分布式缓存，而不要使用本地缓存

1. 种缓存的时候一定种完全，如果缓存数据的一部分有效，一部分无效，宁可放弃种缓存，也不要把部分数据种入缓存
2. 数据库与缓存只需要保持弱一致性，而不需要强一致性，读的顺序要先缓存，后数据库，写的顺序要先数据库，后缓存

这里的最佳实践能够解决*案例8：缓存和数据库不一致、案例9：本地缓存节点间不一致、案例10：缓存数据结构不一致*的问题，对于数据存储层、缓存与数据库、Nosql等的一致性是更深入的存储一致性技术，将会在后续文章单独介绍，这里的数据一致性主要是处理应用层与缓存、应用层与数据库、一部分的缓存与数据库的一致性。

## 超时处理模式





## 迁移开关的设计

在大多数企业里，新项目和老项目一般会共存，大家都在努力的下掉老项目，但是由于种种原因总是下不掉，如果要彻底的下掉老项目，就必须要有非常完善的迁移方案，迁移是一项非常复杂而艰巨的任务，我会在将来的文章中详细探讨迁移方案、流程和技术，这里我们只对迁移中使用的开关进行描述。

迁移过程必须使用开关，开关一般都会基于多个维度来设计，例如：全局的、用户的、角色的、商户的、产品的等等，如果迁移过程中遇到问题，我们需要关闭开关，迁移回老的系统，这需要我们的新系统兼容老的数据，老的系统也兼容新的数据，从某种意义上来讲，迁移比实现新系统更加困难。

曾经看过很多简单的开关设计，有的开关设计在应用层次，通过一个curl语句调用，没有权限控制，这样的开关在服务池的每个节点都是不同步的、不一致的；还有的系统把开关配置放在中心化的配置系统、数据库或者缓存等，处理的每个请求都通过统一的开关来判断是否迁移等等，这样的开关有一个致命的缺点，服务请求在处理过程中，开关可能会变化，各个节点之间开关可能不同步、不一致，导致重复的请求可能走到新的逻辑又走了老的逻辑，如果新的逻辑和老的逻辑没有保证幂等性，这个请求就被重复处理了，如果是金融行业的应用，可能会导致资金损失，电商系统可能会导致发货并退款等问题。

这里面我们推荐使用订单开关，不管我们在什么维度上设计了开关，接收到服务请求后，我们在请求创建的关联实体（例如：订单）上标记开关，以后的任何处理流程，包括同步的和异步的处理流程，都通过订单上的开关来判断，而不是通过全局的或者基于配置的开关，这样在订单创建的时候，开关已经确定，不再变更，一旦一份数据不再发生变化，那么它永远是线程安全的，并且不会有不一致的问题。

这个模式在生产中使用比较频繁，建议每个企业都把这个模式作为设计评审的一项，如果不检查这一项，很多开发童鞋都会偷懒，直接在配置中或者数据库中做个开关就上线了。














